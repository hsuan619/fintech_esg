æª”æ¡ˆæ•´åˆå ±å‘Š
ç›®æ¨™ç›®éŒ„: D:\ç¢©ä¸€\é‡‘èç§‘æŠ€\æ¯”è³½\pepsico
ç›®æ¨™å‰¯æª”å: .py, .yaml, .md
ç”¢ç”Ÿæ™‚é–“: 2025-12-16 13:16:55
==================================================


==================== é–‹å§‹è™•ç†ç›®éŒ„: D:\ç¢©ä¸€\é‡‘èç§‘æŠ€\æ¯”è³½\pepsico ====================

----------- .\app.py -----------

import streamlit as st

from ui import tab_pdf_to_json, tab_risk_assessment


# è¨­å®šé é¢é…ç½® (å¿…é ˆæ˜¯ç¬¬ä¸€å€‹ Streamlit æŒ‡ä»¤)
st.set_page_config(page_title="ESG ç›®æ¨™æ¢å‹˜è€…", layout="wide", page_icon="ğŸŒ±")

# ==========================================
# Streamlit ç¶²é ä»‹é¢ (Web Interface)
# ==========================================

st.title("ğŸŒ± ESG-Goal-Minerï½œESG ç›®æ¨™æ¢å‹˜è€…")
st.markdown(
    """
æœ¬å·¥å…·æä¾›å¾ **PDF å ±å‘Šâ†’ç›®æ¨™ JSON** åˆ° **ç¸¾æ•ˆé¢¨éšªç¨½æ ¸** çš„ä¸€ç«™å¼æµç¨‹ã€‚  
æ­¥é©Ÿ 1ï¼šä¸Šå‚³ PDF è‡ªå‹•æ“·å–æ‰¿è«¾ç›®æ¨™ï¼›æ­¥é©Ÿ 2ï¼šä¸Šå‚³ JSON é€²è¡Œç¸¾æ•ˆè¿½è¹¤èˆ‡é¢¨éšªè©•ä¼°ã€‚
"""
)

# å»ºç«‹å…©å€‹ä¸»è¦é ç±¤
tab1, tab2 = st.tabs(
    [
        "ğŸ“„ 1. ä¸Šå‚³å ±å‘Šä¸¦ç”¢å‡ºç›®æ¨™ JSON (PDF â†’ JSON)",
        "ğŸ“Š 2. ç¸¾æ•ˆè¿½è¹¤èˆ‡é¢¨éšªè©•ä¼°",
    ]
)

# ------------------------------------------
# Tab 1: å ±å‘Šä¸Šå‚³èˆ‡ç›®æ¨™æ“·å– (PDF -> JSON)
# ------------------------------------------
with tab1:
    tab_pdf_to_json.render()

# ------------------------------------------
# Tab 2: ç¸¾æ•ˆè¿½è¹¤èˆ‡é¢¨éšªè©•ä¼° (æ ¸å¿ƒé‚è¼¯å€)
# ------------------------------------------
with tab2:
    tab_risk_assessment.render()


----------- .\app_bk1215.py -----------

import streamlit as st
import tempfile
import os
import re
import pandas as pd
import json
import ast
from datetime import datetime
from markitdown import MarkItDown  # éœ€å®‰è£: pip install markitdown

# è¨­å®šé é¢é…ç½® (å¿…é ˆæ˜¯ç¬¬ä¸€å€‹ Streamlit æŒ‡ä»¤)
st.set_page_config(page_title="ESG æ¼‚ç¶ ç¨½æ ¸å°å¹«æ‰‹", layout="wide", page_icon="ğŸŒ±")

# ==========================================
# æ ¸å¿ƒé‚è¼¯å€ (ç§»æ¤è‡ª app_bk.py ä¸¦å¢å¼·)
# ==========================================

def clean_year(value):
    """å°‡è¼¸å…¥è½‰æ›ç‚ºå¹´ä»½æ•´æ•¸ (e.g. '2015' -> 2015)"""
    try:
        if pd.isna(value) or value == 'None' or value is None: 
            return None
        # è™•ç†å¯èƒ½çš„æµ®é»æ•¸å¹´ä»½æˆ–å­—ä¸²
        return int(float(str(value).split('.')[0]))
    except:
        return None
def clean_value(value):
    """
    é€šç”¨æ•¸å€¼æ¸…æ´— (ä¿®å¾©ç‰ˆ)ï¼š
    1. ä¿®å¾©èª¤åˆ¤æ‹¬è™Ÿç‚ºè² æ•¸çš„å•é¡Œ (ä¾‹å¦‚ "Scope (3)" ä¸æ‡‰è¢«è¦–ç‚ºè² æ•¸)ã€‚
    2. ä¿®å¾©æ•¸å€¼é»é€£å•é¡Œ (ä¾‹å¦‚ "100 tCO2" ä¸æ‡‰è®Š "1002")ã€‚
    3. æ”¯æ´æœƒè¨ˆè² æ•¸æ ¼å¼ (ä¾‹å¦‚ "(5)" -> -5)ã€‚
    """
    try:
        if pd.isna(value) or str(value).lower() == 'none': 
            return None, False, False
            
        str_val = str(value).strip()
        
        # 1. åˆ¤æ–·ç™¾åˆ†æ¯”
        is_percentage = '%' in str_val
        
        # 2. æš«æ™‚ç§»é™¤ % ä»¥ä¾¿å¾ŒçºŒè™•ç†æ•¸å€¼ (é¿å…å¹²æ“¾æ•¸å­—æå–)
        temp_str = str_val.replace('%', '').strip()
        
        # 3. åˆ¤æ–·æ˜¯å¦ç‚ºæœƒè¨ˆè² æ•¸æ ¼å¼ (æ‹¬è™ŸåŒ…ä½ç´”æ•¸å­—)ï¼Œä¾‹å¦‚ (5), (1,000.5)
        # åš´æ ¼æª¢æŸ¥ï¼šæ‹¬è™Ÿå…§åªèƒ½æœ‰æ•¸å­—ã€é€—è™Ÿã€å°æ•¸é»ï¼Œä¸”å¿…é ˆä½”æ“šæ•´å€‹å­—ä¸²(æˆ–å»é ­å»å°¾å¾Œ)
        accounting_pattern = r'^\s*\(\s*([\d,.]+)\s*\)\s*$'
        accounting_match = re.match(accounting_pattern, temp_str)
        
        is_negative_format = False
        float_val = 0.0
        
        if accounting_match:
            # ç¢ºå¯¦æ˜¯ (5) é€™ç¨®æ ¼å¼ -> è¦–ç‚ºè² æ•¸
            is_negative_format = True
            clean_str = accounting_match.group(1).replace(',', '')
            float_val = -abs(float(clean_str))
        else:
            # ä¸€èˆ¬æ ¼å¼ extraction
            # å°‹æ‰¾å­—ä¸²ä¸­ã€Œç¬¬ä¸€å€‹ã€ç¬¦åˆæ•¸å€¼æ ¼å¼çš„éƒ¨åˆ†
            # è¦å‰‡: (é–‹é ­/ç©ºç™½/æ‹¬è™Ÿ) + (å¯é¸è² è™Ÿ) + æ•¸å­—(å«é€—è™Ÿ) + (å¯é¸å°æ•¸é»)
            # é€™æ¨£å¯ä»¥é¿å…æŠ“åˆ° tCO2 ä¸­çš„ 2
            extract_pattern = r'(?:^|[\s\(\[])([-+]?(?:\d{1,3}(?:,\d{3})*|\d+)(?:\.\d+)?)'
            match = re.search(extract_pattern, temp_str)
            
            if match:
                clean_str = match.group(1).replace(',', '')
                float_val = float(clean_str)
                # é€™è£¡ä¸è‡ªå‹•è½‰è² æ•¸ï¼Œé™¤é extract åˆ°çš„æœ¬èº«å°±æœ‰è² è™Ÿ
            else:
                return None, False, False

        # 4. è™•ç†ç™¾åˆ†æ¯”æ•¸å€¼
        if is_percentage:
            return float_val / 100, True, is_negative_format
        
        return float_val, False, is_negative_format

    except Exception:
        return None, False, False
    
def normalize_packaging_scope(scope):
    s = scope.lower().strip()

    if "virgin" in s and "plastic" in s:
        return "virgin_plastic_absolute"

    if "recycled" in s:
        return "recycled_content"

    if "reusable" in s or "recyclable" in s or "compostable" in s:
        return "rrc_packaging"

    return "packaging_general"
    
def calculate_risk(json_data):
    """
    æ ¸å¿ƒé¢¨éšªè¨ˆç®—å‡½å¼ (ç§»æ¤è‡ª app_bk.py)
    æ”¯æ´: ç·šæ€§é æœŸæ³•ã€è·é›¢ç›®æ¨™æ³•ã€çµ•å°å€¼å‹•æ…‹è½‰ç™¾åˆ†æ¯”
    å›å‚³: (DataFrame, è­¦å‘Šåˆ—è¡¨)
    """
    results = []
    warnings = []  # è¿½è¹¤éœ€è¦é¡¯ç¤ºè­¦å‘Šçš„è¨˜éŒ„

    # --- é è™•ç†ï¼šç‚ºæ¯ç­†è³‡æ–™åˆ¤æ–·æ˜¯å¦èˆ‡åŒçµ„çš„å‰ä¸€ç­† target ä¸åŒ ---
    from collections import defaultdict
    change_notes_by_index = {}
    entries = []
    for idx, it in enumerate(json_data):
        f = it.get('Standardized_Focus_Area', 'Unknown')
        m = it.get('Standardized_Metric', 'Unknown')
        s = it.get('Scope', 'Global')
        norm_s = it.get('Normalize_Scope') or it.get('Normalized_Scope') or it.get('NormalizedScope') or it.get('Standardized_Scope') or s
        ry = clean_year(it.get('Report_Year'))
        ty = clean_year(it.get('Target_Deadline'))
        tv = it.get('Target_Value')
        by = clean_year(it.get('Baseline_Year'))
        entries.append({'idx': idx, 'focus': f, 'metric': m, 'norm_scope': norm_s, 'report_year': ry, 'target_year': ty, 'target_val': tv, 'baseline_year': by})

    groups = defaultdict(list)
    for e in entries:
        groups[(e['focus'], e['metric'], e['norm_scope'])].append(e)
    
    for key, lst in groups.items():
        # å…ˆæŒ‰ Report_Year å‡åºæ’åˆ—ï¼Œç¼ºå¹´è€…æ”¾åˆ°æœ€å¾Œï¼ˆç¶­æŒåŸå§‹é †åºï¼‰
        lst_with_year = [e for e in lst if e['report_year'] is not None]
        lst_no_year = [e for e in lst if e['report_year'] is None]
        lst_sorted = sorted(lst_with_year, key=lambda x: x['report_year']) + lst_no_year
        for i in range(1, len(lst_sorted)):
            prev = lst_sorted[i-1]
            cur = lst_sorted[i]
            prev_ty = prev.get('target_year')
            prev_tv = prev.get('target_val')
            cur_ty = cur.get('target_year')
            cur_tv = cur.get('target_val')
            prev_by = prev.get('baseline_year')
            cur_by = cur.get('baseline_year')
            # å…ˆæ¯”è¼ƒ targetï¼ˆdeadline æˆ– valueï¼‰ï¼Œè‹¥ target ç›¸åŒå†æ¯”è¼ƒ baseline year
            if (prev_ty != cur_ty) or (prev_tv != cur_tv):
                change_notes_by_index[cur['idx']] = f"; ç›®æ¨™å·²è®Šæ›´ (å‰: {prev_ty}å¹´ {prev_tv} -> ç¾: {cur_ty}å¹´ {cur_tv})"
                if prev_by != cur_by:
                    change_notes_by_index[cur['idx']] += f"; åŸºæº–å¹´å·²è®Šæ›´ (å‰: {prev_by} -> ç¾: {cur_by})"
            elif prev_by != cur_by:
                change_notes_by_index[cur['idx']] = f"åŸºæº–å¹´å·²è®Šæ›´ (å‰: {prev_by} -> ç¾: {cur_by})"
    # --- ä¸»è¦è¨ˆç®—æµç¨‹ ---
    
    for idx, item in enumerate(json_data):
        try:
            # --- A. åŸºç¤è³‡æ–™è®€å– ---
            focus_area = item.get('Standardized_Focus_Area', 'Unknown')
            metric = item.get('Standardized_Metric', 'Unknown')
            scope = item.get('Scope', 'Global')
            report_year = clean_year(item.get('Report_Year'))
            
            # è®€å–ç›®æ¨™ (Target)
            target_year = clean_year(item.get('Target_Deadline'))
            target_val_str = item.get('Target_Value')
            # å„ªå…ˆå˜—è©¦å¯èƒ½çš„æ¨™æº–åŒ– scope æ¬„ä½
            norm_scope = item.get('Normalize_Scope') or item.get('Normalized_Scope') or item.get('NormalizedScope') or item.get('Standardized_Scope') or scope
            # å¾é è™•ç†çµæœä¸­å–å¾—è®Šæ›´å‚™è¨»ï¼ˆè‹¥æœ‰ï¼‰
            change_note = change_notes_by_index.get(idx, "")
            
            # è®€å–åŸºæº–å¹´ (Baseline)
            base_year = clean_year(item.get('Baseline_Year'))
            
            # ç›®æ¨™é€šå¸¸æ˜¯ç™¾åˆ†æ¯”ï¼Œå¼·åˆ¶è¦–ç‚ºç™¾åˆ†æ¯”è™•ç†
            target_reduction, _, _ = clean_value(target_val_str)
            
            # å¦‚æœç›®æ¨™æ²’å¯«%ï¼Œä½†æ•¸å€¼æ¯”å¦‚æ˜¯ 20ï¼Œé€šå¸¸æŒ‡ 20% (0.2)
            if target_reduction is not None and target_reduction > 1: 
                target_reduction /= 100
            
            # --- B. è§£æé€²åº¦æ­·å² (Progress History) ---
            history_str = item.get('Progress_History', '[]')
            try:
                if isinstance(history_str, list):
                    history_list = history_str
                else:
                    history_list = ast.literal_eval(history_str)
            except:
                history_list = []
            
            if not history_list:
                results.append({
                    "Focus_Area": focus_area, "Metric": metric, "Report_Year": report_year,
                    "Risk_Level": "æ•¸æ“šä¸è¶³", "Analysis_Note": "ç„¡æ­·å²é€²åº¦æ•¸æ“š", "Target": f"{target_year}å¹´ {target_val_str}",
                    "Has_Negative_Warning": False,
                    "Target_Change_Note": change_note
                })
                continue
            
            # æ•´ç†æ­·å²æ•¸æ“š
            history_map = {}
            valid_history = []
            has_negative_warning = False  # è¿½è¹¤æ˜¯å¦æœ‰è² æ•¸è­¦å‘Š
            
            for h in history_list:
                y = clean_year(h.get('Year'))
                raw_v = h.get('Value')
                v, is_pct, is_negative_fmt = clean_value(raw_v)
                
                if y is not None and v is not None:
                    record = {'Year': y, 'Value': v, 'Is_Pct': is_pct, 'Raw': raw_v, 'Is_Negative_Fmt': is_negative_fmt}
                    valid_history.append(record)
                    history_map[y] = record
                    # å¦‚æœæœ€æ–°å¹´ä»½æœ‰è² æ•¸æ ¼å¼è­¦å‘Š
                    if is_negative_fmt:
                        has_negative_warning = True
            
            if not valid_history:
                results.append({
                    "Focus_Area": focus_area, "Metric": metric, "Report_Year": report_year, "Scope": scope,
                    "Risk_Level": "æ•¸æ“šä¸è¶³", "Note": "ç„¡æ­·å²é€²åº¦æ•¸æ“š", "Target": f"{target_year}å¹´ {target_val_str}",
                    "Current_Status": "N/A",
                    "Has_Negative_Warning": False, "Target_Change_Note": change_note, "Analysis_Note": ""
                })
                continue
            
            valid_history.sort(key=lambda x: x['Year'])
            latest_record = valid_history[-1]
            Y_current = latest_record['Year']
            
            # å¦‚æœç¼ºå°‘åŸºæº–å¹´ï¼Œä½†æœ‰æ­·å²æ•¸æ“šï¼Œé¡¯ç¤ºè©²å¹´åº¦çš„æ¸›é‡ç‹€æ³
            if base_year is None:
                actual_reduction = latest_record['Value']
                results.append({
                    "Focus_Area": focus_area, "Metric": metric, "Report_Year": report_year, "Scope": scope,
                    "Risk_Level": "æ•¸æ“šä¸è¶³",  "Target": f"{target_year}å¹´ {target_val_str}",
                    "Current_Status": f"{Y_current}å¹´ (æ¸›é‡ {actual_reduction:.1%})" if actual_reduction is not None else "N/A",
                    "Has_Negative_Warning": False, "Target_Change_Note": change_note, "Analysis_Note": "ç„¡æ³•è¨ˆç®—é¢¨éšªï¼ˆç¼ºå°‘åŸºæº–å¹´ï¼‰"
                })
                continue
            
            # --- C. è¨ˆç®—å¯¦éš›æ¸›é‡ (Actual Reduction) ---
            actual_reduction = 0.0
            calc_method = ""
            
            # åˆ¤æ–·æ˜¯ç”¨ã€Œçµ•å°å€¼ã€ç®—é‚„æ˜¯ç›´æ¥æ‹¿ã€Œç™¾åˆ†æ¯”ã€
            if not latest_record['Is_Pct']:
                # æƒ…å¢ƒ 1: æ­·å²æ•¸æ“šæ˜¯ã€Œçµ•å°æ•¸å€¼ã€(Absolute Value)
                if base_year in history_map:
                    base_val = history_map[base_year]['Value']
                    curr_val = latest_record['Value']
                    
                    if base_val != 0:
                        # å…¬å¼: (åŸºæº– - ç¾åœ¨) / åŸºæº–
                        actual_reduction = (base_val - curr_val) / base_val
                        calc_method = f"çµ•å°å€¼è¨ˆç®— (åŸºæº–{base_year}: {base_val:,.0f} -> {Y_current}: {curr_val:,.0f})"
                    else:
                        results.append({"Focus_Area": focus_area, "Metric": metric, "Report_Year": report_year, "Risk_Level": "æ•¸æ“šéŒ¯èª¤", "Analysis_Note": "åŸºæº–å¹´æ’æ”¾é‡ç‚º 0", "Has_Negative_Warning": False, "Target_Change_Note": change_note})
                        continue
                else:
                    results.append({
                        "Focus_Area": focus_area, "Metric": metric, "Report_Year": report_year,
                        "Risk_Level": "æ•¸æ“šä¸è¶³", "Target": f"{target_year}å¹´ {target_val_str}",
                        "Analysis_Note": f"æ­·å²æ•¸æ“šç‚ºçµ•å°å€¼ï¼Œä½†åœ¨ History ä¸­æ‰¾ä¸åˆ°åŸºæº–å¹´ ({base_year}) çš„æ•¸æ“šã€‚",
                        "Has_Negative_Warning": False,
                        "Target_Change_Note": change_note
                    })
                    continue
            else:
                # æƒ…å¢ƒ 2: æ­·å²æ•¸æ“šæœ¬èº«å°±æ˜¯ã€Œæ¸›é‡ç™¾åˆ†æ¯”ã€
                actual_reduction = latest_record['Value']
                calc_method = "ç›´æ¥è®€å–ç™¾åˆ†æ¯”"

            # --- D. æ ¸å¿ƒæ¼”ç®—æ³• (Risk Logic) ---
            total_years = target_year - base_year
            elapsed_years = Y_current - base_year

            if total_years <= 0:
                results.append({"Focus_Area": focus_area, "Metric": metric, "Report_Year": report_year, "Risk_Level": "è¨­å®šéŒ¯èª¤", "Analysis_Note": "ç›®æ¨™å¹´æ—©æ–¼åŸºæº–å¹´", "Has_Negative_Warning": False, "Target_Change_Note": change_note})
                continue
            
            elapsed_years = max(0, elapsed_years)

            # æ–¹æ³•ä¸€ï¼šç·šæ€§é æœŸé€²åº¦æ³•
            expected_progress = (elapsed_years / total_years) * target_reduction
            
            if expected_progress > 0:
                gap = (expected_progress - actual_reduction) / expected_progress
            else:
                gap = 0
            
            flag1 = gap > 0.1 # è½å¾Œ 10% ä»¥ä¸Š
            flag3 = gap > 1.0 # è½å¾Œ 100% ä»¥ä¸Š

            # æ–¹æ³•äºŒï¼šè·é›¢ç›®æ¨™æ³•
            time_ratio = elapsed_years / total_years
            target_ratio = actual_reduction / target_reduction if target_reduction > 0 else 0
            
            flag2 = (time_ratio >= 0.5 and target_ratio < 0.5)

            # --- E. é¢¨éšªåˆ¤å®š ---
            if (flag1 and flag2) or flag3:
                risk_level = "ğŸ”´ é«˜åº¦é¢¨éšª"
            elif flag1 or flag2:
                risk_level = "ğŸŸ  ä¸­åº¦é¢¨éšª"
            else:
                risk_level = "ğŸŸ¢ ä½é¢¨éšª"

            # --- F. ç”¢ç”Ÿå‚™è¨» ---
            if risk_level.startswith("ğŸŸ¢"):
                note = f"é€²åº¦ç¬¦åˆé æœŸã€‚{calc_method}"
            else:
                note = (
                    f"æ‡‰æ¸› {expected_progress:.1%}, å¯¦æ¸› {actual_reduction:.1%} (Gap: {gap:.1%})ã€‚ "
                    f"{calc_method}"
                )


            result_item = {
                "Focus_Area": focus_area,
                "Report_Year": report_year,
                "Metric": metric,
                "Scope": scope,
                "Target": f"{target_year}å¹´ {target_val_str}",
                "Current_Status": f"{Y_current}å¹´ (æ¸›é‡ {actual_reduction:.1%})",
                "Risk_Level": risk_level,
                "Analysis_Note": note,
                "Has_Negative_Warning": has_negative_warning and actual_reduction < 0,
                "Target_Change_Note": change_note
            }
            results.append(result_item)
            
            # å¦‚æœæœ‰è² æ•¸è­¦å‘Šï¼Œæ·»åŠ åˆ°è­¦å‘Šåˆ—è¡¨
            if result_item["Has_Negative_Warning"]:
                warnings.append({
                    "Focus_Area": focus_area,
                    "Metric": metric,
                    "Year": Y_current,
                    "Status": actual_reduction
                })

        except Exception as e:
            results.append({
                "Focus_Area": item.get('Standardized_Focus_Area'),
                "Metric": item.get('Standardized_Metric'),
                "Report_Year": item.get('Report_Year'),
                "Risk_Level": "è¨ˆç®—éŒ¯èª¤",
                "Note": str(e),
                "Current_Status": "N/A",
                "Target": "N/A",
                "Analysis_Note": "N/A",
                "Scope": "N/A",
                "Has_Negative_Warning": False
            })

    return pd.DataFrame(results), warnings


def get_audit_prompt(current_year, content):
    # æ³¨æ„ï¼šé€™è£¡ä½¿ç”¨ {{ }} ä¾†è½‰ç¾© JSON çš„å¤§æ‹¬è™Ÿï¼Œä»¥ä¾¿ f-string æ­£ç¢ºé‹ä½œ
    template = f"""
# Role
ä½ æ˜¯ä¸€ä½ç²¾é€š ESG å ±å‘Šæ¨™æº– (å¦‚ GRI, SASB) çš„ç¨½æ ¸å“¡ã€‚ä½ çš„ä»»å‹™æ˜¯å¾ä¼æ¥­æ°¸çºŒå ±å‘Šæ›¸ä¸­æå–ã€Œæ‰¿è«¾ç›®æ¨™ã€ï¼Œä¸¦ä¾æ“šå…§å»ºçš„æ¨™æº–åŒ–å­—å…¸é€²è¡Œåˆ†é¡ï¼Œä»¥ä¾¿é€²è¡Œè·¨å¹´åº¦æ•¸æ“šæ¯”å°ã€‚

# Context
**ç›®å‰æ­£åœ¨è™•ç†çš„å ±å‘Šå¹´ä»½**: {current_year}

# Input Data
ä½ å°‡è§£ææä¾›çš„è¡¨æ ¼åœ–ç‰‡æˆ–æ–‡å­—ã€‚é€™äº›æ•¸æ“šä¾†è‡ªä¸Šè¿°å¹´ä»½çš„æ°¸çºŒå ±å‘Šæ›¸ã€‚

# Task 1: Extraction & Standardization (æå–èˆ‡æ¨™æº–åŒ–)
è«‹å°‡æå–å‡ºçš„ç›®æ¨™æ˜ å°„åˆ°ä»¥ä¸‹çš„æ¨™æº–åŒ–éšå±¤çµæ§‹ã€‚å¦‚æœä¸å®Œå…¨åŒ¹é…ï¼Œè«‹é¸æ“‡èªæ„æœ€æ¥è¿‘çš„é¸é …ã€‚

## ğŸ“š Standardized ESG Dictionary (æ¨™æº–åŒ–å­—å…¸)

### 1. ğŸŒ Focus Area: Climate (æ°£å€™è®Šé·)
   - **Target Metrics**: 
     - `Absolute GHG Reduction` (æº«å®¤æ°£é«”çµ•å°æ¸›é‡)
     - `Net Zero` (æ·¨é›¶æ’æ”¾)
     - `Renewable Energy` (å†ç”Ÿèƒ½æºæ¯”ä¾‹)
     - `Energy Efficiency` (èƒ½æºä½¿ç”¨æ•ˆç‡)
   - **Typical Scopes**: `Scope 1+2`, `Scope 3`, `Value Chain`, `Global Operations`
   - **Strict Formatting Rule**: 
     - ç•¶ç›®æ¨™æ¶‰åŠ Scope 1 èˆ‡ Scope 2 æ™‚ï¼ŒOutput Scope æ¬„ä½è«‹**åš´æ ¼**å¡«å…¥ `Scope 1+2`ã€‚
     - **ç¦æ­¢**åœ¨ Scope æ¬„ä½åŠ å…¥åœ°å€ã€å­å…¬å¸æˆ–æ‹¬è™Ÿè¨»è¨˜ (ä¾‹å¦‚ï¼š**ä¸è¦å¯«** `Scope 1+2 (Taiwan Operations)` æˆ– `Scope 1+2 (Company only)`ï¼Œä¸€å¾‹åˆªé™¤æ‹¬è™Ÿå…§å®¹ï¼Œåªä¿ç•™ `Scope 1+2`)ã€‚

### 2. ğŸ“¦ Focus Area: Packaging (åŒ…è£èˆ‡å¾ªç’°ç¶“æ¿Ÿ)
   - **Target Metrics**: 
     - `Recycled Content` (å†ç”Ÿæ–™ä½¿ç”¨æ¯”ä¾‹, e.g., rPET)
     - `Virgin Plastic Reduction` (åŸç”Ÿå¡‘è† æ¸›é‡)
     - `Packaging Design` (å¯å›æ”¶/å¯å †è‚¥è¨­è¨ˆ, e.g., Recyclability)
     - `Reuse Models` (é‡è¤‡ä½¿ç”¨æ¨¡å¼/æ¸›é‡)
     - `Waste to Landfill` (å»¢æ£„ç‰©æ©åŸ‹ç‡)
   - **Typical Scopes**: `Plastic Packaging`, `Beverage Containers`, `Food Packaging`, `Global Portfolio`

### 3. ğŸ’§ Focus Area: Water (æ°´è³‡æº)
   - **Target Metrics**: 
     - `Water Replenishment` (æ°´è³‡æºå›è£œ)
     - `Water Use Efficiency` (ç”¨æ°´æ•ˆç‡/å¼·åº¦)
   - **Typical Scopes**: `High Water-Risk Areas`, `Manufacturing Operations`

### 4. ğŸŒ± Focus Area: Agriculture (æ°¸çºŒè¾²æ¥­)
   - **Target Metrics**: 
     - `Regenerative Agriculture` (å†ç”Ÿè¾²æ¥­æ¡ç”¨é¢ç©)
     - `Sustainably Sourced` (æ°¸çºŒæ¡è³¼æ¯”ä¾‹)
   - **Typical Scopes**: `Key Ingredients`, `Direct Supply Chain`

### 5. ğŸ‘¥ Focus Area: Human Rights & Social (äººæ¬Šèˆ‡ç¤¾æœƒ)
   - **Target Metrics**: 
     - `Gender Diversity` (æ€§åˆ¥å¤šæ¨£æ€§/ç®¡ç†å±¤æ¯”ä¾‹)
     - `Safety` (å·¥å‚·ç‡/å®‰å…¨äº‹æ•…)
     - `Human Rights Audit` (äººæ¬Šç›¡è·èª¿æŸ¥)
   - **Typical Scopes**: `Global Workforce`, `Management Roles`, `Tier 1 Suppliers`
# Task 2: Data Cleaning Rules (è³‡æ–™æ¸…æ´—è¦å‰‡)
1. **å¿½ç•¥æ­·å²æ•¸æ“š**: è«‹å¿½ç•¥æ‰€æœ‰å°æ–¼æˆ–ç­‰æ–¼ 2022 çš„é€²åº¦æ•¸å€¼ (Results/Status)ï¼Œæˆ‘å€‘åªé—œå¿ƒã€Œæœªä¾†çš„ç›®æ¨™ (Target)ã€ã€‚
2. **Deadline Logic**:
   - å„ªå…ˆæª¢æŸ¥ç›®æ¨™æè¿°å…§çš„å¹´ä»½ (å¦‚ "by 2025")ã€‚
   - è‹¥ç„¡ï¼Œå‰‡ä½¿ç”¨è¡¨é ­å¹´ä»½ (å¦‚ "2030 Target")ã€‚
3. **Value Parsing**: åªæå–ç›®æ¨™æ•¸å€¼ (å¦‚ "100%", "50%")ï¼Œå»é™¤æ–‡å­—æ•˜è¿°ã€‚
4. **Baseline Logic (åŸºæº–å¹´åˆ¤å®šç­–ç•¥)**:
   - **ç›´æ¥æè¿°**: å„ªå…ˆæª¢æŸ¥ç›®æ¨™æ–‡å­—ä¸­æ˜¯å¦åŒ…å« "vs. 20XX baseline" æˆ– "from a 20XX base"ã€‚
   - **è¨»è…³é—œè¯ (Footnote & Superscript)**: æª¢æŸ¥ç›®æ¨™æ–‡å­—æˆ–**è©²å€å¡Šæ¨™é¡Œ**æ—é‚Šæ˜¯å¦æœ‰ä¸Šæ¨™æ•¸å­— (å¦‚ `[1]`, `1`)ã€‚è‹¥æœ‰ï¼Œè«‹å‹™å¿…æª¢ç´¢è¡¨æ ¼åº•éƒ¨æˆ–é å°¾çš„è¨»è…³æ–‡å­— (Footnotes/Comments)ï¼Œé€šå¸¸åŸºæº–å¹´æœƒå®šç¾©åœ¨é‚£è£¡ (ä¾‹å¦‚ "Measured versus a 2020 baseline")ã€‚
   - **å±¤ç´šç¹¼æ‰¿ (Hierarchy Inheritance)**: è‹¥è©²æŒ‡æ¨™ (e.g., Recycled Content) å±¬æ–¼ä¸€å€‹å¤§ç›®æ¨™ (Parent Goal, e.g., Virgin Plastic Reduction) çš„å­é …ï¼Œä¸”å¤§ç›®æ¨™æˆ–å€å¡Šæ¨™é¡Œæœ‰æ˜ç¢ºåŸºæº–å¹´ï¼Œè«‹**ç¹¼æ‰¿**è©²åŸºæº–å¹´ã€‚
   - **æœ€å¾Œæ‰‹æ®µ**: è‹¥ä»¥ä¸Šçš†ç„¡ï¼Œæ‰è€ƒæ…®ä½¿ç”¨ Progress_History ä¸­æœ€æ—©é‚£å¹´ - 1ã€‚

# Output JSON Schema
è«‹è¼¸å‡ºä¸€å€‹ JSON Listï¼š
[
  {{
    "Report_Year": {current_year},
"Standardized_Focus_Area": "String (e.g., 'Packaging', 'Climate')",
    "Standardized_Metric": "String (e.g., 'Recycled Content')",
    
    // Level 3: é©ç”¨ç¯„ç–‡/æè³ª
    // è¦å‰‡ï¼š
    // 1. å„ªå…ˆå¾è¨»è…³æˆ–æ¨™é¡Œæå–ä¸»è¦ç¯„ç–‡ (å¦‚ "Primary plastic packaging")ã€‚
    // 2. è‹¥ Original_Goal_Text ä¸­æ˜ç¢ºæåŠè¡¡é‡æ–¹å¼ (å¦‚ "absolute tonnage", "per serving")ï¼Œè«‹å‹™å¿…è£œå……åœ¨æ‹¬è™Ÿå…§ã€‚
    // ç¯„ä¾‹è¼¸å…¥: "Reduce absolute tonnage... of primary plastic" -> è¼¸å‡º: "Primary plastic packaging (absolute tonnage)"
    "Scope": "String",
    
    "Original_Goal_Text": "String (ä¿ç•™å ±å‘Šä¸­çš„å®Œæ•´åŸå§‹æè¿°)",
    "Target_Deadline": Number (e.g., 2025, 2030),
    "Target_Value": "String (e.g., '25%', '50%', 'Net Zero')",
    
    // Baseline æå–æ³¨æ„ï¼šè«‹å‹™å¿…æª¢æŸ¥ä¸Šæ¨™(superscript)å°æ‡‰çš„è¨»è…³ï¼Œæˆ–å¤§æ¨™é¡Œçš„åŸºæº–å¹´è¨­å®š
    "Baseline_Year": "String (è‹¥æœ‰æåŠåŸºæº–å¹´å‰‡å¡«å…¥ï¼Œå¦å‰‡ null)",
    "Progress_History": [
       {{ "Year": Number, "Value": "String" }}
    ]
  }}
]

# Begin Extraction
è«‹åˆ†æä»¥ä¸‹å…§å®¹ï¼š
{content}
"""
    return template

# ==========================================
# 3. Streamlit ç¶²é ä»‹é¢ (Web Interface)
# ==========================================

st.title("ğŸŒ± ESG å ±å‘Šæ›¸æ¼‚ç¶ ç¨½æ ¸å°å¹«æ‰‹")
st.markdown("""
æœ¬å·¥å…·æä¾›å¾ **PDF å ±å‘Šè½‰æ›** åˆ° **ç¸¾æ•ˆé¢¨éšªç¨½æ ¸** çš„ä¸€ç«™å¼æµç¨‹ã€‚
è«‹ä¾åºä½¿ç”¨ä¸‹æ–¹åˆ†é åŠŸèƒ½ï¼š
""")

# å»ºç«‹ä¸‰å€‹ä¸»è¦é ç±¤
tab1, tab2, tab3 = st.tabs([
    "ğŸ“„ 1. å ±å‘Šè½‰æ› (PDF to MD)", 
    "ğŸ¤– 2. ç”¢ç”Ÿç¨½æ ¸ Prompt", 
    "ğŸ“Š 3. ç¸¾æ•ˆè¿½è¹¤èˆ‡é¢¨éšªè©•ä¼°"
])

# ------------------------------------------
# Tab 1: å ±å‘Šè½‰æ› (PDF -> Markdown)
# ------------------------------------------
with tab1:
    st.header("æ­¥é©Ÿä¸€ï¼šä¸Šå‚³ä¸¦è½‰æ›å ±å‘Šæ›¸")
    st.markdown("å°‡ PDF æ ¼å¼çš„ ESG å ±å‘Šæ›¸è½‰æ›ç‚º AI å¯è®€çš„ Markdown æ ¼å¼ã€‚")
    
    uploaded_pdf = st.file_uploader("ä¸Šå‚³ ESG å ±å‘Šæ›¸ (PDF)", type=["pdf"], key="pdf_uploader")
    
    # ç‹€æ…‹ä¿å­˜ï¼šMarkdown å…§å®¹
    if 'markdown_content' not in st.session_state:
        st.session_state.markdown_content = ""
    
    # å˜—è©¦å¾æª”åè‡ªå‹•æå–å¹´ä»½
    default_year = 2024
    if uploaded_pdf:
        match = re.search(r'20\d{2}', uploaded_pdf.name)
        if match:
            default_year = int(match.group(0))
            
    report_year = st.number_input("è¨­å®šå ±å‘Šå¹´ä»½", min_value=2000, max_value=2030, value=default_year, key="report_year_input")
    st.session_state.report_year = report_year

    if uploaded_pdf is not None:
        if st.button("é–‹å§‹è½‰æ›"):
            st.info(f"æ­£åœ¨è™•ç†æª”æ¡ˆ: {uploaded_pdf.name} ...")
            
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_pdf:
                tmp_pdf.write(uploaded_pdf.read())
                tmp_pdf_path = tmp_pdf.name

            try:
                md = MarkItDown()
                result = md.convert(tmp_pdf_path)
                st.session_state.markdown_content = result.text_content
                os.remove(tmp_pdf_path)
                st.success("è½‰æ›æˆåŠŸï¼è«‹è‡³ã€Œç”¢ç”Ÿç¨½æ ¸ Promptã€åˆ†é æŸ¥çœ‹ã€‚")
                
            except Exception as e:
                st.error(f"è½‰æ›éŒ¯èª¤: {e}")
                if os.path.exists(tmp_pdf_path): os.remove(tmp_pdf_path)
    
    if st.session_state.markdown_content:
        with st.expander("æŸ¥çœ‹è½‰æ›å¾Œçš„ Markdown å…§å®¹"):
            st.text_area("å…§å®¹é è¦½", st.session_state.markdown_content, height=300)
            st.download_button(
                label="ä¸‹è¼‰ Markdown (.md)",
                data=st.session_state.markdown_content,
                file_name=f"report_{report_year}.md",
                mime="text/markdown"
            )

# ------------------------------------------
# Tab 2: ç”¢ç”Ÿç¨½æ ¸ Prompt
# ------------------------------------------
with tab2:
    st.header("æ­¥é©ŸäºŒï¼šç”Ÿæˆ AI ç¨½æ ¸ Prompt")
    st.markdown("å°‡è½‰æ›å¾Œçš„å…§å®¹çµåˆæ¨™æº–åŒ–æŒ‡ä»¤ï¼Œç”¢ç”Ÿå¯ä¾› ChatGPT/Claude/Gemini ä½¿ç”¨çš„ Promptã€‚")
    
    if st.session_state.markdown_content:
        final_prompt = get_audit_prompt(st.session_state.report_year, st.session_state.markdown_content)
        
        st.info("ğŸ’¡ è«‹è¤‡è£½ä¸‹æ–¹å…§å®¹ï¼Œè²¼çµ¦ LLM æ¨¡å‹ï¼Œä¸¦å°‡å…¶å›å‚³çš„ JSON å­˜æª”ä¾›æ­¥é©Ÿä¸‰ä½¿ç”¨ã€‚")
        st.text_area("Prompt é è¦½", final_prompt, height=400)
        
        st.download_button(
            label="ä¸‹è¼‰å®Œæ•´ Prompt (.txt)",
            data=final_prompt,
            file_name=f"Audit_Prompt_{st.session_state.report_year}.txt",
            mime="text/plain"
        )
    else:
        st.warning("è«‹å…ˆåœ¨æ­¥é©Ÿä¸€ä¸Šå‚³ä¸¦è½‰æ› PDF å ±å‘Šã€‚")

# ------------------------------------------
# Tab 3: ç¸¾æ•ˆè¿½è¹¤èˆ‡é¢¨éšªè©•ä¼° (æ ¸å¿ƒé‚è¼¯å€)
# ------------------------------------------
with tab3:
    st.header("æ­¥é©Ÿä¸‰ï¼šç¸¾æ•ˆè¿½è¹¤èˆ‡é¢¨éšªè©•ä¼°")
    st.markdown("""
    è«‹ä¸Šå‚³ç”± LLM ç”¢å‡ºçš„ **çµæ§‹åŒ– JSON æª”æ¡ˆ**ã€‚
    ç³»çµ±å°‡è‡ªå‹•åŸ·è¡Œï¼š
    1. **è§£ææ­·å¹´æ•¸æ“š** (æ”¯æ´çµ•å°å€¼è½‰ç™¾åˆ†æ¯”ï¼Œè‡ªå‹•å°‡ `(5)%` è½‰ç‚º `-5%`)ã€‚
    2. **å°æ‡‰åŸºæº–å¹´** (Baseline Mapping)ã€‚
    3. **è¨ˆç®—é¢¨éšªç­‰ç´š** (ç·šæ€§é æœŸæ³• + è·é›¢ç›®æ¨™æ³•)ã€‚
    """)
    
    uploaded_json = st.file_uploader("ä¸Šå‚³ LLM ç”¢å‡ºçš„ JSON æª”æ¡ˆ", type=["json"], key="json_uploader")

    if uploaded_json is not None:
        try:
            # è®€å– JSON
            json_data = json.load(uploaded_json)
            st.success(f"æˆåŠŸè®€å–æª”æ¡ˆï¼å…± {len(json_data)} ç­†ç›®æ¨™è³‡æ–™ã€‚")
            
            # åŸ·è¡Œè¨ˆç®—
            with st.spinner('æ­£åœ¨é€²è¡Œé¢¨éšªè©•ä¼°æ¼”ç®—æ³•...'):
                df_result, warnings_list = calculate_risk(json_data)
            
            # é¡¯ç¤ºè­¦å‘Šå½ˆçª— (Current_Status èƒŒé“è€Œé¦³)
            if warnings_list:
                for warn in warnings_list:
                    st.error(
                        f"âŒå¹´åº¦: {warn['Year']} - {warn['Focus_Area']} - {warn['Metric']}\n\n"
                        f"è©²å¹´åº¦çš„æ¸›é‡ç‹€æ³ç‚ºè² æ•¸ ({warn['Status']:.1%})ï¼Œ"
                        f"èˆ‡ç›®æ¨™èƒŒé“è€Œé¦³ï¼"
                    )

            # é¡¯ç¤ºçµæœ
            st.subheader("ğŸ“Š ç¨½æ ¸åˆ†æçµæœ")

            # ç›´æ¥é¡¯ç¤ºè¡¨æ ¼ï¼ˆéš±è—å…§éƒ¨æ¬„ä½ Has_Negative_Warningï¼‰
            df_display = df_result.drop(columns=['Has_Negative_Warning'], errors='ignore')
            # ä¾ Report_Year é è¨­å‡åºæ’åºï¼ˆè‹¥æœ‰æ­¤æ¬„ä½ï¼‰
            if 'Report_Year' in df_display.columns:
                try:
                    df_display = df_display.sort_values(by='Report_Year', ascending=True)
                except Exception:
                    pass
            st.dataframe(df_display, use_container_width=True)

            # ä¸‹è¼‰ CSV
            # åœ¨å°å‡ºå‰ç§»é™¤ Has_Negative_Warning åˆ—
            df_export = df_result.drop(columns=['Has_Negative_Warning'], errors='ignore')
            csv = df_export.to_csv(index=False, encoding='utf-8-sig')
            base_name = uploaded_json.name.replace(".json", "")
            file_name = f"Audit_Result_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{base_name}.csv"
            
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰å®Œæ•´åˆ†æå ±è¡¨ (CSV)",
                data=csv,
                file_name=file_name,
                mime="text/csv",
            )

        except Exception as e:
            st.error(f"åˆ†æéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            st.info("è«‹ç¢ºèªä¸Šå‚³çš„ JSON æ ¼å¼æ˜¯å¦ç¬¦åˆ Prompt å®šç¾©çš„ Schemaã€‚")
    else:
        st.info("ğŸ‘‹ ç­‰å¾…ä¸Šå‚³ JSON æª”æ¡ˆä¸­...")

----------- .\esg_goal_miner.py -----------

"""
ESG-Goal-Miner
===============

ç”¨æ³•ï¼ˆCLIï¼‰:

    cd pepsico
    python esg_goal_miner.py --pdf pdf/2023-ESG-Performance-Metrics.pdf --year 2023 --output All_json/2023.json

è¼¸å…¥:
    - ä¸€ä»½ ESG PDF å ±å‘Š
    - å ±å‘Šå¹´ä»½ current_yearï¼ˆæ‰‹å‹•æŒ‡å®šï¼Œé¿å…è‡ªå‹•åˆ¤æ–·å‡ºéŒ¯ï¼‰

è¼¸å‡º:
    - ä¸€å€‹ JSON æª”æ¡ˆï¼Œå…§å®¹ç‚ºæ•´ä»½å ±å‘Šæ‰€æœ‰é é¢ä¸­åµæ¸¬åˆ°çš„ã€Œæ‰¿è«¾ç›®æ¨™ã€åˆ—è¡¨ã€‚
      çµæ§‹éµå®ˆ core.prompt.get_audit_prompt å®šç¾©çš„ Schemaã€‚
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Any, Dict, List

from core.gemini_client import GeminiClient
from core.pdf_extractor import extract_mixed_content


def run_esg_goal_miner(pdf_path: Path, report_year: int, output_path: Path) -> None:
    pages = extract_mixed_content(str(pdf_path))

    client = GeminiClient()
    all_items: List[Dict[str, Any]] = []

    for page in pages:
        text: str = page["text"]
        images: List[bytes] = page["images"]

        page_items = client.extract_goals_from_page(
            page_text=text,
            images=images,
            current_year=report_year,
        )

        for item in page_items:
            if isinstance(item, dict):
                item.setdefault("Report_Year", report_year)
                all_items.append(item)

    output_path.parent.mkdir(parents=True, exist_ok=True)
    with output_path.open("w", encoding="utf-8") as f:
        json.dump(all_items, f, ensure_ascii=False, indent=2)

    print(f"[ESG-Goal-Miner] å…±å¯«å‡º {len(all_items)} ç­†ç›®æ¨™è‡³: {output_path}")


def _parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="ESG-Goal-Miner: å¾ ESG PDF å ±å‘Šè‡ªå‹•æŠ½å–æ‰¿è«¾ç›®æ¨™ç‚º JSON")
    parser.add_argument(
        "--pdf",
        type=str,
        required=True,
        help="è¼¸å…¥ PDF æª”æ¡ˆè·¯å¾‘ï¼Œä¾‹å¦‚: pdf/2023-ESG-Performance-Metrics.pdf",
    )
    parser.add_argument(
        "--year",
        type=int,
        required=True,
        help="å ±å‘Šå¹´ä»½ (current_year)ï¼Œä¾‹å¦‚ 2023",
    )
    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="è¼¸å‡º JSON æª”æ¡ˆè·¯å¾‘ï¼Œä¾‹å¦‚: All_json/2023.json",
    )
    return parser.parse_args()


def main() -> None:
    args = _parse_args()
    pdf_path = Path(args.pdf)
    output_path = Path(args.output)

    if not pdf_path.is_file():
        raise SystemExit(f"æ‰¾ä¸åˆ° PDF æª”æ¡ˆ: {pdf_path}")

    run_esg_goal_miner(pdf_path=pdf_path, report_year=args.year, output_path=output_path)


if __name__ == "__main__":
    main()




----------- .\note.md -----------

##conda evn
activate test1126


##usage pdf to filename.md
markitdown path-to-file.pdf -o document.md

markitdown 2024-ESG-Performance-Metrics.pdf -o 2024-ESG-Performance-Metrics.md


##è·‘ç¶²é 
streamlit run app.py



##éƒ¨ç½²
https://github.com/hsuan619/fintech_esg.git

----------- .\pp.py -----------

import fitz  # PyMuPDF
import google.generativeai as genai
from typing import List, Dict, Union, Tuple

def extract_content_smart(pdf_path: str, page_num: int) -> Tuple[str, List[bytes], bool]:
    """
    æ™ºæ…§æå–ï¼šè‡ªå‹•åˆ¤æ–·æ˜¯å‚³é€æ–‡å­—é‚„æ˜¯å‚³é€æ•´é åœ–ç‰‡ã€‚
    
    Returns:
        text_content (str): æå–çš„æ–‡å­—
        images (List[bytes]): åœ–ç‰‡åˆ—è¡¨ (å¦‚æœæ˜¯æƒææª”ï¼Œé€™è£¡æœƒåŒ…å«æ•´é æˆªåœ–)
        is_scanned_mode (bool): æ˜¯å¦å•Ÿç”¨äº†æƒææ¨¡å¼ (é™¤éŒ¯ç”¨)
    """
    doc = fitz.open(pdf_path)
    page = doc.load_page(page_num)
    
    text = page.get_text()
    images_to_send = []
    is_scanned_mode = False

    # === åˆ¤æ–·é‚è¼¯ ===
    # å¦‚æœæ–‡å­—å¤ªå°‘ (ä¾‹å¦‚å°‘æ–¼ 1000 å­—)ï¼Œæˆ‘å€‘å‡è¨­å®ƒæ˜¯ï¼š
    # 1. æƒææª” (Scanned PDF)
    # 2. å…¨ç‰ˆå¤§åœ–/æµ·å ± (Infographic)
    # é€™æ™‚æˆ‘å€‘å•Ÿå‹•ã€Œè¦–è¦ºæ¨¡å¼ã€ï¼ŒæŠŠæ•´é è½‰æˆåœ–ç‰‡
    if len(text.strip()) < 1000:
        is_scanned_mode = True
        
        # å°‡æ•´é æ¸²æŸ“ç‚ºé«˜è§£æåº¦åœ–ç‰‡ (zoom=2 ä»£è¡¨ 200% è§£æåº¦ï¼ŒOCR æº–åº¦è¼ƒé«˜)
        pix = page.get_pixmap(matrix=fitz.Matrix(2, 2)) 
        img_bytes = pix.tobytes("png")
        images_to_send.append(img_bytes)
        
        # åœ¨æƒææ¨¡å¼ä¸‹ï¼ŒText è¨­ç‚ºæç¤ºèªï¼Œå‘Šè¨´ AI é€™æ˜¯ä¸€å¼µæˆªåœ–
        text = "[ç³»çµ±æç¤º: æ­¤é é¢ç‚ºæƒææª”æˆ–å…¨ç‰ˆåœ–è¡¨ï¼Œè«‹ç›´æ¥åˆ†æé™„åœ–å…§å®¹]"
        
    else:
        # === ä¸€èˆ¬æ¨¡å¼ (æ–‡å­—ç‰ˆ PDF) ===
        # é›–ç„¶æœ‰æ–‡å­—ï¼Œä½†å¯èƒ½é‚„æœ‰æ’åœ– (Bar Chart ç­‰)ï¼Œä¹Ÿè¦æŠ“å‡ºä¾†
        image_list = page.get_images(full=True)
        for img_index, img in enumerate(image_list):
            try:
                xref = img[0]
                base_image = doc.extract_image(xref)
                images_to_send.append(base_image["image"])
            except Exception:
                continue # å¿½ç•¥æå£çš„åœ–ç‰‡

    doc.close()
    return text, images_to_send, is_scanned_mode

----------- .\process_files.py -----------

# -*- coding: utf-8 -*-

import os
import argparse
from datetime import datetime

# --- å…¨åŸŸè¨­å®š: åœ¨æ­¤è™•æ–°å¢æ‚¨æƒ³åœ¨ã€Œæ–‡ä»¶æ¨¹ã€ä¸­å¿½ç•¥çš„è³‡æ–™å¤¾æˆ–æª”æ¡ˆåç¨± ---
# ä½¿ç”¨é›†åˆ(set)å¯ä»¥æé«˜æŸ¥æ‰¾æ•ˆç‡
IGNORED_PATTERNS = {
    '__pycache__',  # Python çš„å¿«å–è³‡æ–™å¤¾ (ä½¿ç”¨è€…è¦æ±‚)
    '.git',         # Git ç‰ˆæœ¬æ§åˆ¶è³‡æ–™å¤¾
    'venv',         # Python è™›æ“¬ç’°å¢ƒè³‡æ–™å¤¾
    '.vscode',      # VSCode ç·¨è¼¯å™¨è¨­å®šè³‡æ–™å¤¾
    '.idea',        # JetBrains IDEs è¨­å®šè³‡æ–™å¤¾
    '.env',         # ç’°å¢ƒè®Šæ•¸æª”æ¡ˆ
    'node_modules', # Node.js ä¾è³´åº«è³‡æ–™å¤¾
    'build',        # ç·¨è­¯è¼¸å‡ºè³‡æ–™å¤¾
    'dist',         # æ‰“åŒ…è¼¸å‡ºè³‡æ–™å¤¾
    'process_files.py',
    "train",
    "test",
    "val",
    ".json",
    ".pdf"
}


def combine_files(directory_paths, extensions, output_file_path):
    """
    éè¿´æƒæå¤šå€‹æŒ‡å®šç›®éŒ„ï¼Œå°‡æ‰€æœ‰ç¬¦åˆæŒ‡å®šå‰¯æª”ååˆ—è¡¨çš„æª”æ¡ˆæ•´åˆæˆå–®ä¸€æ–‡ä»¶ã€‚
    """
    extensions_tuple = tuple(extensions)
    print(f"[*] é–‹å§‹æ•´åˆå‰¯æª”åç‚º '{', '.join(extensions)}' çš„æª”æ¡ˆ...")
    try:
        with open(output_file_path, 'w', encoding='utf-8') as outfile:
            # å¯«å…¥ç¸½å ±å‘Šé ­
            outfile.write(f"æª”æ¡ˆæ•´åˆå ±å‘Š\n")
            outfile.write(f"ç›®æ¨™ç›®éŒ„: {', '.join([os.path.abspath(p) for p in directory_paths])}\n")
            outfile.write(f"ç›®æ¨™å‰¯æª”å: {', '.join(extensions)}\n")
            outfile.write(f"ç”¢ç”Ÿæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            outfile.write("="*50 + "\n")

            # éæ­·æ¯ä¸€å€‹æŒ‡å®šçš„ç›®éŒ„
            for directory_path in directory_paths:
                abs_dir_path = os.path.abspath(directory_path)
                print(f"\n[+] æ­£åœ¨è™•ç†ç›®éŒ„: {abs_dir_path}")
                outfile.write(f"\n\n{'='*20} é–‹å§‹è™•ç†ç›®éŒ„: {abs_dir_path} {'='*20}\n\n")

                found_files = False
                for root, _, files in os.walk(directory_path):
                    # å¿½ç•¥è¢«æŒ‡å®šçš„è³‡æ–™å¤¾
                    if any(ignored in root.split(os.sep) for ignored in IGNORED_PATTERNS):
                        continue
                        
                    for file in sorted(files):
                        if file.endswith(extensions_tuple):
                            found_files = True
                            file_path = os.path.join(root, file)
                            print(f"  -> æ­£åœ¨è®€å–: {file_path}")
                            outfile.write(f"----------- {file_path} -----------\n\n")
                            try:
                                with open(file_path, 'r', encoding='utf-8', errors='ignore') as infile:
                                    content = infile.read()
                                    outfile.write(content)
                                    outfile.write("\n\n")
                            except Exception as e:
                                outfile.write(f"*** ç„¡æ³•è®€å–æª”æ¡ˆ: {e} ***\n\n")
                
                if not found_files:
                    print(f"  -> åœ¨ {abs_dir_path} ä¸­æœªæ‰¾åˆ°ç¬¦åˆå‰¯æª”åçš„æª”æ¡ˆã€‚")
                    outfile.write(f"*** åœ¨æ­¤ç›®éŒ„ä¸­æœªæ‰¾åˆ°ç¬¦åˆå‰¯æª”å '{', '.join(extensions)}' çš„æª”æ¡ˆ ***\n\n")


        print(f"\n[+] æˆåŠŸï¼æ‰€æœ‰æª”æ¡ˆå·²æ•´åˆè‡³: {output_file_path}\n")

    except Exception as e:
        print(f"[!] æ•´åˆæª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\n")


def generate_tree(directory_paths, output_file_path, ignored=None):
    """
    ç‚ºå¤šå€‹æŒ‡å®šç›®éŒ„ç”¢ç”Ÿæ–‡ä»¶æ¨¹çµæ§‹ï¼Œä¸¦å°‡æ‰€æœ‰æ¨¹ç‹€åœ–å„²å­˜è‡³å–®ä¸€ txt æª”ã€‚
    """
    if ignored is None:
        ignored = set()
    
    print(f"[*] é–‹å§‹ç”¢ç”Ÿæ–‡ä»¶æ¨¹ (å°‡å¿½ç•¥: {', '.join(ignored)})...")
    try:
        with open(output_file_path, 'w', encoding='utf-8') as f:
            f.write(f"å¤šç›®éŒ„æ–‡ä»¶æ¨¹å ±å‘Š\n")
            f.write(f"ç”¢ç”Ÿæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            if ignored:
                f.write(f"(å…¨åŸŸå¿½ç•¥è¦å‰‡: {', '.join(sorted(list(ignored)))})\n")
            f.write("="*50 + "\n")

            # éæ­·æ¯ä¸€å€‹æŒ‡å®šçš„ç›®éŒ„
            for directory_path in directory_paths:
                abs_dir_path = os.path.abspath(directory_path)
                print(f"\n[+] æ­£åœ¨ç‚ºç›®éŒ„ç”¢ç”Ÿæ¨¹ç‹€åœ–: {abs_dir_path}")
                f.write(f"\n\n{'='*20} ç›®éŒ„æ¨¹: {abs_dir_path} {'='*20}\n")
                
                f.write(f"{os.path.basename(abs_dir_path)}\n")
                _create_tree_recursive(f, abs_dir_path, "", ignored)
        
        print(f"\n[+] æˆåŠŸï¼æ‰€æœ‰æ–‡ä»¶æ¨¹å·²å„²å­˜è‡³: {output_file_path}\n")

    except Exception as e:
        print(f"[!] ç”¢ç”Ÿæ–‡ä»¶æ¨¹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\n")


def _create_tree_recursive(file_obj, dir_path, prefix="", ignored=None):
    """
    éè¿´è¼”åŠ©å‡½å¼ï¼Œç”¨ä¾†å»ºç«‹æ–‡ä»¶æ¨¹çš„æ¯ä¸€å±¤çµæ§‹ï¼Œæœƒè·³éå¿½ç•¥åˆ—è¡¨ä¸­çš„é …ç›®ã€‚
    (æ­¤å‡½å¼åŠŸèƒ½ä¸è®Š)
    """
    if ignored is None:
        ignored = set()

    try:
        # å–å¾—ç›®éŒ„ä¸‹æ‰€æœ‰é …ç›®ï¼Œä¸¦é å…ˆéæ¿¾æ‰è¦å¿½ç•¥çš„æª”æ¡ˆ/è³‡æ–™å¤¾
        items = [item for item in os.listdir(dir_path) if item not in ignored]
        # å°‡è³‡æ–™å¤¾æ’åœ¨å‰é¢
        entries = sorted(items, key=lambda x: not os.path.isdir(os.path.join(dir_path, x)))
    except OSError as e:
        file_obj.write(f"{prefix}â””â”€â”€ [ç„¡æ³•å­˜å–: {e}]\n")
        return

    for i, entry in enumerate(entries):
        connector = "â””â”€â”€ " if i == len(entries) - 1 else "â”œâ”€â”€ "
        file_obj.write(f"{prefix}{connector}{entry}\n")

        entry_path = os.path.join(dir_path, entry)
        if os.path.isdir(entry_path):
            new_prefix = "    " if i == len(entries) - 1 else "â”‚   "
            _create_tree_recursive(file_obj, entry_path, prefix + new_prefix, ignored)


def main():
    """
    ä¸»å‡½å¼ï¼Œè™•ç†ä½¿ç”¨è€…è¼¸å…¥ä¸¦åŸ·è¡Œä¸»è¦åŠŸèƒ½ã€‚
    """
    parser = argparse.ArgumentParser(
        description="ä¸€å€‹å¼·å¤§çš„æ–‡ä»¶è™•ç†å·¥å…·ï¼Œå¯ä»¥æ•´åˆæŒ‡å®šå‰¯æª”åçš„æª”æ¡ˆï¼Œä¸¦ç‚ºå¤šå€‹ç›®éŒ„ç”¢ç”Ÿæ–‡ä»¶æ¨¹ã€‚",
        formatter_class=argparse.RawTextHelpFormatter
    )
    
    parser.add_argument(
        "directories", 
        nargs='+',  # æ¥å—ä¸€å€‹æˆ–å¤šå€‹å€¼
        help="è¦è™•ç†çš„ä¸€å€‹æˆ–å¤šå€‹ç›®æ¨™è³‡æ–™å¤¾è·¯å¾‘ï¼Œè«‹ç”¨ç©ºæ ¼åˆ†éš”ã€‚"
    )
    parser.add_argument(
        "-e", "--extensions", 
        nargs='+',
        default=['.py', '.yaml', '.md'], 
        help="è¦æ•´åˆçš„ä¸€å€‹æˆ–å¤šå€‹æª”æ¡ˆå‰¯æª”åï¼Œè«‹ç”¨ç©ºæ ¼åˆ†éš”ã€‚\nç¯„ä¾‹: -e .py .html .css (é è¨­å€¼: .py)"
    )
    parser.add_argument("-o1", "--output_combine", default="all_files.txt", help="æ•´åˆå¾Œçš„æ–‡ä»¶åç¨±ã€‚(é è¨­å€¼: combine_files.txt)")
    parser.add_argument("-o2", "--output_tree", default="trees.txt", help="æ–‡ä»¶æ¨¹çš„è¼¸å‡ºæ–‡ä»¶åç¨±ã€‚(é è¨­å€¼: trees.txt)")
    
    args = parser.parse_args()

    directory_paths = args.directories
    extensions = args.extensions
    output_combine_path = args.output_combine
    output_tree_path = args.output_tree

    # é©—è­‰æ‰€æœ‰æä¾›çš„è·¯å¾‘éƒ½æ˜¯æœ‰æ•ˆçš„è³‡æ–™å¤¾
    for path in directory_paths:
        if not os.path.isdir(path):
            print(f"[!] éŒ¯èª¤ï¼šæŒ‡å®šçš„è·¯å¾‘ '{path}' ä¸æ˜¯ä¸€å€‹æœ‰æ•ˆçš„è³‡æ–™å¤¾æˆ–ä¸å­˜åœ¨ã€‚")
            return

    # å‚³å…¥ç›®éŒ„åˆ—è¡¨é€²è¡Œè™•ç†
    combine_files(directory_paths, extensions, output_combine_path)
    generate_tree(directory_paths, output_tree_path, ignored=IGNORED_PATTERNS)
    
    print("âœ¨ æ‰€æœ‰ä»»å‹™åŸ·è¡Œå®Œç•¢ï¼ âœ¨")

#python process_files.py ./

if __name__ == "__main__":
    main()

----------- .\core\__init__.py -----------

"""
æ ¸å¿ƒå•†æ¥­é‚è¼¯æ¨¡çµ„åŒ…ï¼š
- cleaning: é€šç”¨æ¸…æ´—èˆ‡å‰è™•ç†
- risk: é¢¨éšªè¨ˆç®—ä¸»æµç¨‹
- prompt: LLM ç¨½æ ¸ Prompt ç”¢ç”Ÿå™¨
"""




----------- .\core\cleaning.py -----------

import re
from typing import Any, Optional, Tuple

import pandas as pd


def clean_year(value: Any) -> Optional[int]:
    """å°‡è¼¸å…¥è½‰æ›ç‚ºå¹´ä»½æ•´æ•¸ (e.g. '2015' -> 2015)ï¼›å¤±æ•—å‰‡å›å‚³ Noneã€‚"""
    try:
        if pd.isna(value) or value == "None" or value is None:
            return None
        # è™•ç†å¯èƒ½çš„æµ®é»æ•¸å¹´ä»½æˆ–å­—ä¸²
        return int(float(str(value).split(".")[0]))
    except Exception:
        return None


def clean_value(value: Any) -> Tuple[Optional[float], bool, bool]:
    """
    é€šç”¨æ•¸å€¼æ¸…æ´— (ä¿®å¾©ç‰ˆ)ï¼š
    1. ä¿®å¾©èª¤åˆ¤æ‹¬è™Ÿç‚ºè² æ•¸çš„å•é¡Œ (ä¾‹å¦‚ "Scope (3)" ä¸æ‡‰è¢«è¦–ç‚ºè² æ•¸)ã€‚
    2. ä¿®å¾©æ•¸å€¼é»é€£å•é¡Œ (ä¾‹å¦‚ "100 tCO2" ä¸æ‡‰è®Š "1002")ã€‚
    3. æ”¯æ´æœƒè¨ˆè² æ•¸æ ¼å¼ (ä¾‹å¦‚ "(5)" -> -5)ã€‚

    Returns:
        (æ•¸å€¼, æ˜¯å¦ç‚ºç™¾åˆ†æ¯”æ ¼å¼, æ˜¯å¦ç‚ºæœƒè¨ˆè² æ•¸æ ¼å¼)
    """
    try:
        if pd.isna(value) or str(value).lower() == "none":
            return None, False, False

        str_val = str(value).strip()

        # 1. åˆ¤æ–·ç™¾åˆ†æ¯”
        is_percentage = "%" in str_val

        # 2. æš«æ™‚ç§»é™¤ % ä»¥ä¾¿å¾ŒçºŒè™•ç†æ•¸å€¼ (é¿å…å¹²æ“¾æ•¸å­—æå–)
        temp_str = str_val.replace("%", "").strip()

        # 3. åˆ¤æ–·æ˜¯å¦ç‚ºæœƒè¨ˆè² æ•¸æ ¼å¼ (æ‹¬è™ŸåŒ…ä½ç´”æ•¸å­—)
        accounting_pattern = r"^\s*\(\s*([\d,.]+)\s*\)\s*$"
        accounting_match = re.match(accounting_pattern, temp_str)

        is_negative_format = False
        float_val = 0.0

        if accounting_match:
            # ç¢ºå¯¦æ˜¯ (5) é€™ç¨®æ ¼å¼ -> è¦–ç‚ºè² æ•¸
            is_negative_format = True
            clean_str = accounting_match.group(1).replace(",", "")
            float_val = -abs(float(clean_str))
        else:
            # ä¸€èˆ¬æ ¼å¼ extraction
            # å°‹æ‰¾å­—ä¸²ä¸­ã€Œç¬¬ä¸€å€‹ã€ç¬¦åˆæ•¸å€¼æ ¼å¼çš„éƒ¨åˆ†
            extract_pattern = (
                r"(?:^|[\s\(\[])([-+]?(?:\d{1,3}(?:,\d{3})*|\d+)(?:\.\d+)?)"
            )
            match = re.search(extract_pattern, temp_str)

            if match:
                clean_str = match.group(1).replace(",", "")
                float_val = float(clean_str)
            else:
                return None, False, False

        # 4. è™•ç†ç™¾åˆ†æ¯”æ•¸å€¼
        if is_percentage:
            return float_val / 100, True, is_negative_format

        return float_val, False, is_negative_format

    except Exception:
        return None, False, False


def normalize_packaging_scope(scope: str) -> str:
    """æ¨™æº–åŒ–åŒ…è£ç›¸é—œ Scope å­—ä¸²ã€‚"""
    s = scope.lower().strip()

    if "virgin" in s and "plastic" in s:
        return "virgin_plastic_absolute"

    if "recycled" in s:
        return "recycled_content"

    if "reusable" in s or "recyclable" in s or "compostable" in s:
        return "rrc_packaging"

    return "packaging_general"




----------- .\core\gemini_client.py -----------

import json
import os
from typing import Any, Dict, List

import google.generativeai as genai
from dotenv import load_dotenv

from .prompt import get_audit_prompt


class GeminiClient:
    """
    è¼•é‡å°è£ Google Gemini 1.5 Flash

    - è‡ªå‹•å¾ .env / ç’°å¢ƒè®Šæ•¸è®€å– GOOGLE_API_KEY
    - æä¾›åœ–ç‰‡å‰è™•ç† (åœ–è¡¨æ–‡å­—æè¿°)
    - ä»¥ JSON mode å›å‚³çµæ§‹åŒ–çµæœ
    """

    def __init__(
        self,
        model_name: str = "gemini-2.5-flash-lite",
        vision_model_name: str | None = None,
        api_key: str | None = None,
    ) -> None:
        load_dotenv()

        api_key = api_key or os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise RuntimeError(
                "ç’°å¢ƒè®Šæ•¸ GOOGLE_API_KEY æœªè¨­å®šï¼Œè«‹åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„å»ºç«‹ .env ä¸¦åŠ å…¥ï¼š\n"
                "GOOGLE_API_KEY=ä½ çš„_API_Key"
            )

        genai.configure(api_key=api_key)

        self._model = genai.GenerativeModel(model_name)
        # è¦–è¦ºå‰è™•ç†å¯ç”¨ä¸åŒ modelï¼›è‹¥æœªæŒ‡å®šå‰‡å…±ç”¨åŒä¸€å€‹
        self._vision_model = (
            genai.GenerativeModel(vision_model_name)
            if vision_model_name
            else self._model
        )

    def _describe_images(self, images: List[bytes]) -> str:
        """
        ä½¿ç”¨ Vision æ¨¡å‹å…ˆæŠŠåœ–è¡¨ã€Œç¿»è­¯æˆæ–‡å­—ã€ï¼Œç‰¹åˆ¥æ˜¯æŠ½å‡ºå¹´åº¦ + æ•¸å€¼çš„æ­·å²è¶¨å‹¢ã€‚
        å›å‚³ï¼šå–®ä¸€é•·æ–‡å­—æè¿°ï¼Œå¯ç›´æ¥æ‹¼åœ¨ content å¾Œé¢ã€‚
        """
        if not images:
            return ""

        parts: List[Any] = [
            (
                "ä½ æ˜¯ä¸€ä½ ESG å ±å‘Šç¨½æ ¸å“¡ï¼Œè«‹å°ˆæ³¨è§£æ**åœ–è¡¨æˆ–è¶¨å‹¢åœ–**ä¸­çš„ï¼š\n"
                "1. ç›®æ¨™å€¼ (Target) èˆ‡ç›®æ¨™å¹´åº¦ (ä¾‹å¦‚ 2025å¹´ 2,467 è¬å™¸, 2030å¹´ 2,271 è¬å™¸)\n"
                "2. æ­·å²é€²åº¦ (Results / Status)ï¼šè«‹ç›¡å¯èƒ½åˆ—å‡ºæ‰€æœ‰ã€Œå¹´åº¦ + æ•¸å€¼ã€çš„è³‡æ–™é»\n"
                "   - è«‹ç”¨é¡ä¼¼ `2005: 2,567 è¬å™¸`, `2006: 2,710 è¬å™¸` çš„æ ¼å¼é€å¹´åˆ—å‡º\n"
                "3. åŸºæº–å¹´ (Baseline) ä»¥åŠèˆ‡åŸºæº–å¹´çš„æ¸›é‡ç™¾åˆ†æ¯” (è‹¥æœ‰æ¨™ç¤ºï¼Œä¾‹å¦‚ã€Œæ¯” 2007 å¹´æ¸›å°‘ 22%ã€)\n\n"
                "è«‹ç”¨æ¢åˆ—å¼è¼¸å‡ºï¼Œæ¯å€‹åœ–è¡¨ä¸€æ®µï¼Œå‹™å¿…æŠŠä½ èƒ½çœ‹è¦‹çš„æ‰€æœ‰å¹´åº¦èˆ‡æ•¸å€¼éƒ½å¯«å‡ºä¾†ã€‚"
            )
        ]

        for img in images:
            parts.append(
                {
                    "mime_type": "image/png",
                    "data": img,
                }
            )

        response = self._vision_model.generate_content(parts)
        return response.text or ""

    def extract_goals_from_page(
        self,
        *,
        page_text: str,
        images: List[bytes],
        current_year: int,
    ) -> List[Dict[str, Any]]:
        """
        æ ¸å¿ƒæ–¹æ³•ï¼šçµ¦ä¸€é  PDF çš„æ–‡å­— + åœ–ç‰‡ï¼Œå›å‚³è©²é æ‰€æœ‰ã€Œæ‰¿è«¾ç›®æ¨™ã€çš„ JSON listã€‚
        """
        image_desc = self._describe_images(images)

        # å°‡åŸå§‹æ–‡å­—èˆ‡åœ–ç‰‡æè¿°åˆä½µï¼Œä¸Ÿçµ¦ Prompt æ¨¡æ¿
        merged_content_parts: List[str] = ["# Page Text", page_text.strip()]
        if image_desc:
            merged_content_parts.extend(
                [
                    "",
                    "# Image-derived Details",
                    image_desc.strip(),
                ]
            )

        merged_content = "\n".join(merged_content_parts)
        prompt = get_audit_prompt(current_year=current_year, content=merged_content)

        response = self._model.generate_content(
            [prompt],
            generation_config=genai.types.GenerationConfig(
                response_mime_type="application/json"
            ),
        )

        raw_text = response.text or "[]"

        try:
            data = json.loads(raw_text)
        except json.JSONDecodeError:
            # è‹¥æ¨¡å‹å¤–å±¤å¤šåŒ…ä¸€å±¤ç‰©ä»¶ï¼Œå˜—è©¦å¹¾å€‹å¸¸è¦‹ key
            try:
                obj = json.loads(raw_text)
                for key in ("items", "data", "results"):
                    if key in obj and isinstance(obj[key], list):
                        return obj[key]
                # å¦‚æœæ˜¯å–®ä¸€ç‰©ä»¶è€Œé listï¼Œå°±åŒ…æˆ list
                if isinstance(obj, dict):
                    return [obj]
                if isinstance(obj, list):
                    return obj
            except Exception:
                # æœ€å¾Œé€€è·¯ï¼šç›´æ¥å›å‚³ç©º listï¼Œé¿å…æ•´é«”æµç¨‹ä¸­æ–·
                return []

        if isinstance(data, dict):
            return [data]
        if isinstance(data, list):
            return data
        return []


__all__ = ["GeminiClient"]




----------- .\core\pdf_extractor.py -----------

from typing import Any, Dict, List

# æ”¯æ´å…©ç¨®åŸ·è¡Œæ–¹å¼ï¼š
# 1. ä½œç‚º package åŒ¯å…¥ï¼š  from pepsico.core.pdf_extractor import ...
# 2. ç›´æ¥åœ¨ pepsico ç›®éŒ„åŸ·è¡Œï¼špython esg_goal_miner.py
try:
    from ..pp import extract_content_smart  # type: ignore[import-not-found]
except ImportError:  # ç•¶å‰ç›®éŒ„ç›´æ¥åŸ·è¡Œæ™‚èµ°é€™æ¢
    from pp import extract_content_smart  # type: ignore[import-not-found]


def extract_mixed_content(pdf_path: str) -> List[Dict[str, Any]]:
    """
    Step 2: PDF æ··åˆå…§å®¹æå–å™¨

    - ä½¿ç”¨ PyMuPDF é€é è®€å–
    - æå–ç´”æ–‡å­—
    - æå–è©²é æ‰€æœ‰åœ–ç‰‡ (bytes)

    å›å‚³æ ¼å¼:
        [
          {
            "page_index": 0,
            "text": "...",
            "images": [b"...", ...],
            "is_scanned": bool,
          },
          ...
        ]
    """
    # extract_content_smart å…§éƒ¨å·²è² è²¬é–‹å•Ÿèˆ‡é—œé–‰ PDF
    pages: List[Dict[str, Any]] = []

    import fitz  # lazy import to keep dependency localized

    doc = fitz.open(pdf_path)
    page_count = doc.page_count
    doc.close()

    for i in range(page_count):
        text, images, is_scanned = extract_content_smart(pdf_path, i)

        # ç‚ºäº†ç¢ºä¿ LLM ä¸€å®šçœ‹åˆ°æ•´å¼µåœ–è¡¨/è¶¨å‹¢åœ–ï¼Œ
        # éæƒææ¨¡å¼ä¸‹å†é¡å¤–è£œä¸Šä¸€å¼µæ•´é æˆªåœ–ã€‚
        if not is_scanned:
            doc_page = fitz.open(pdf_path).load_page(i)
            pix = doc_page.get_pixmap(matrix=fitz.Matrix(1.5, 1.5))
            images.append(pix.tobytes("png"))

        pages.append(
            {
                "page_index": i,
                "text": text,
                "images": images,
                "is_scanned": is_scanned,
            }
        )

    return pages




----------- .\core\prompt.py -----------

def get_audit_prompt(current_year: int, content: str) -> str:
    """
    ç”¢ç”Ÿ ESG æ¼‚ç¶ ç¨½æ ¸ç”¨çš„ LLM Promptã€‚

    åƒæ•¸:
        current_year: å ±å‘Šå¹´ä»½
        content: Markdown æˆ–æ–‡å­—å½¢å¼çš„å ±å‘Šå…§å®¹
    """
    # æ³¨æ„ï¼šé€™è£¡ä½¿ç”¨ {{ }} ä¾†è½‰ç¾© JSON çš„å¤§æ‹¬è™Ÿï¼Œä»¥ä¾¿ f-string æ­£ç¢ºé‹ä½œ
    template = f"""
# Role
ä½ æ˜¯ä¸€ä½ç²¾é€š ESG å ±å‘Šæ¨™æº– (å¦‚ GRI, SASB) çš„ç¨½æ ¸å“¡ã€‚ä½ çš„ä»»å‹™æ˜¯å¾ä¼æ¥­æ°¸çºŒå ±å‘Šæ›¸ä¸­æå–ã€Œæ‰¿è«¾ç›®æ¨™ã€ï¼Œä¸¦ä¾æ“šå…§å»ºçš„æ¨™æº–åŒ–å­—å…¸é€²è¡Œåˆ†é¡ï¼Œä»¥ä¾¿é€²è¡Œè·¨å¹´åº¦æ•¸æ“šæ¯”å°ã€‚

# Context
**ç›®å‰æ­£åœ¨è™•ç†çš„å ±å‘Šå¹´ä»½**: {current_year}

# Input Data
ä½ å°‡è§£ææä¾›çš„è¡¨æ ¼åœ–ç‰‡æˆ–æ–‡å­—ã€‚é€™äº›æ•¸æ“šä¾†è‡ªä¸Šè¿°å¹´ä»½çš„æ°¸çºŒå ±å‘Šæ›¸ã€‚

# Task 1: Extraction & Standardization (æå–èˆ‡æ¨™æº–åŒ–)
è«‹å°‡æå–å‡ºçš„ç›®æ¨™æ˜ å°„åˆ°ä»¥ä¸‹çš„æ¨™æº–åŒ–éšå±¤çµæ§‹ã€‚å¦‚æœä¸å®Œå…¨åŒ¹é…ï¼Œè«‹é¸æ“‡èªæ„æœ€æ¥è¿‘çš„é¸é …ã€‚

## ğŸ“š Standardized ESG Dictionary (æ¨™æº–åŒ–å­—å…¸)

### 1. ğŸŒ Focus Area: Climate (æ°£å€™è®Šé·)
   - **Target Metrics**: 
     - `Absolute GHG Reduction` (æº«å®¤æ°£é«”çµ•å°æ¸›é‡)
     - `Net Zero` (æ·¨é›¶æ’æ”¾)
     - `Renewable Energy` (å†ç”Ÿèƒ½æºæ¯”ä¾‹)
     - `Energy Efficiency` (èƒ½æºä½¿ç”¨æ•ˆç‡)
   - **Typical Scopes**: `Scope 1+2`, `Scope 3`, `Value Chain`, `Global Operations`
   - **Strict Formatting Rule**: 
     - ç•¶ç›®æ¨™æ¶‰åŠ Scope 1 èˆ‡ Scope 2 æ™‚ï¼ŒOutput Scope æ¬„ä½è«‹**åš´æ ¼**å¡«å…¥ `Scope 1+2`ã€‚
     - **ç¦æ­¢**åœ¨ Scope æ¬„ä½åŠ å…¥åœ°å€ã€å­å…¬å¸æˆ–æ‹¬è™Ÿè¨»è¨˜ (ä¾‹å¦‚ï¼š**ä¸è¦å¯«** `Scope 1+2 (Taiwan Operations)` æˆ– `Scope 1+2 (Company only)`ï¼Œä¸€å¾‹åˆªé™¤æ‹¬è™Ÿå…§å®¹ï¼Œåªä¿ç•™ `Scope 1+2`)ã€‚

### 2. ğŸ“¦ Focus Area: Packaging (åŒ…è£èˆ‡å¾ªç’°ç¶“æ¿Ÿ)
   - **Target Metrics**: 
     - `Recycled Content` (å†ç”Ÿæ–™ä½¿ç”¨æ¯”ä¾‹, e.g., rPET)
     - `Virgin Plastic Reduction` (åŸç”Ÿå¡‘è† æ¸›é‡)
     - `Packaging Design` (å¯å›æ”¶/å¯å †è‚¥è¨­è¨ˆ, e.g., Recyclability)
     - `Reuse Models` (é‡è¤‡ä½¿ç”¨æ¨¡å¼/æ¸›é‡)
     - `Waste to Landfill` (å»¢æ£„ç‰©æ©åŸ‹ç‡)
   - **Typical Scopes**: `Plastic Packaging`, `Beverage Containers`, `Food Packaging`, `Global Portfolio`

### 3. ğŸ’§ Focus Area: Water (æ°´è³‡æº)
   - **Target Metrics**: 
     - `Water Replenishment` (æ°´è³‡æºå›è£œ)
     - `Water Use Efficiency` (ç”¨æ°´æ•ˆç‡/å¼·åº¦)
   - **Typical Scopes**: `High Water-Risk Areas`, `Manufacturing Operations`

### 4. ğŸŒ± Focus Area: Agriculture (æ°¸çºŒè¾²æ¥­)
   - **Target Metrics**: 
     - `Regenerative Agriculture` (å†ç”Ÿè¾²æ¥­æ¡ç”¨é¢ç©)
     - `Sustainably Sourced` (æ°¸çºŒæ¡è³¼æ¯”ä¾‹)
   - **Typical Scopes**: `Key Ingredients`, `Direct Supply Chain`

### 5. ğŸ‘¥ Focus Area: Human Rights & Social (äººæ¬Šèˆ‡ç¤¾æœƒ)
   - **Target Metrics**: 
     - `Gender Diversity` (æ€§åˆ¥å¤šæ¨£æ€§/ç®¡ç†å±¤æ¯”ä¾‹)
     - `Safety` (å·¥å‚·ç‡/å®‰å…¨äº‹æ•…)
     - `Human Rights Audit` (äººæ¬Šç›¡è·èª¿æŸ¥)
   - **Typical Scopes**: `Global Workforce`, `Management Roles`, `Tier 1 Suppliers`
# Task 2: Data Cleaning Rules (è³‡æ–™æ¸…æ´—è¦å‰‡)
1. **æ­·å²é€²åº¦èˆ‡è¶¨å‹¢æ•¸æ“šæŠ½å– (Progress_History)**:
   - ç•¶åœ–è¡¨æˆ–è¡¨æ ¼ä¸­å‡ºç¾ã€Œå¹´åº¦ + æ•¸å€¼ã€çš„è¶¨å‹¢ç·šæˆ–é•·æ¢åœ– (ä¾‹å¦‚ 2005~2030 å¹´æ’æ”¾é‡è¶¨å‹¢)ï¼Œè«‹**ç›¡å¯èƒ½æŠ½å–æ‰€æœ‰å¯ä»¥è¾¨è­˜çš„å¹´åº¦èˆ‡å°æ‡‰æ•¸å€¼**ã€‚
   - é€™äº›å¹´åº¦èˆ‡æ•¸å€¼è«‹ä¸€å¾‹å¡«å…¥ `Progress_History` æ¬„ä½ã€‚
   - å³ä½¿é€™äº›å¹´ä»½å°æ–¼æˆ–ç­‰æ–¼ {current_year}ï¼Œä¹Ÿ**ä¸è¦å¿½ç•¥**ï¼Œå› ç‚ºå¾ŒçºŒé¢¨éšªè¨ˆç®—éœ€è¦å®Œæ•´çš„æ­·å²è¶¨å‹¢ã€‚
2. **Deadline Logic**:
   - å„ªå…ˆæª¢æŸ¥ç›®æ¨™æè¿°å…§çš„å¹´ä»½ (å¦‚ "by 2025")ã€‚
   - è‹¥ç„¡ï¼Œå‰‡ä½¿ç”¨è¡¨é ­å¹´ä»½ (å¦‚ "2030 Target")ã€‚
3. **Value Parsing**: åªæå–ç›®æ¨™æ•¸å€¼ (å¦‚ "100%", "50%", "2,467 è¬å™¸")ï¼Œå»é™¤ç„¡é—œæ•˜è¿°ï¼Œä½†å¯ä¿ç•™å¿…è¦å–®ä½ã€‚
4. **Baseline Logic (åŸºæº–å¹´åˆ¤å®šç­–ç•¥)**:
   - **ç›´æ¥æè¿°**: å„ªå…ˆæª¢æŸ¥ç›®æ¨™æ–‡å­—ä¸­æ˜¯å¦åŒ…å« "vs. 20XX baseline" æˆ– "from a 20XX base"ã€‚
   - **è¨»è…³é—œè¯ (Footnote & Superscript)**: æª¢æŸ¥ç›®æ¨™æ–‡å­—æˆ–**è©²å€å¡Šæ¨™é¡Œ**æ—é‚Šæ˜¯å¦æœ‰ä¸Šæ¨™æ•¸å­— (å¦‚ `[1]`, `1`)ã€‚è‹¥æœ‰ï¼Œè«‹å‹™å¿…æª¢ç´¢è¡¨æ ¼åº•éƒ¨æˆ–é å°¾çš„è¨»è…³æ–‡å­— (Footnotes/Comments)ï¼Œé€šå¸¸åŸºæº–å¹´æœƒå®šç¾©åœ¨é‚£è£¡ (ä¾‹å¦‚ "Measured versus a 2020 baseline")ã€‚
   - **å±¤ç´šç¹¼æ‰¿ (Hierarchy Inheritance)**: è‹¥è©²æŒ‡æ¨™ (e.g., Recycled Content) å±¬æ–¼ä¸€å€‹å¤§ç›®æ¨™ (Parent Goal, e.g., Virgin Plastic Reduction) çš„å­é …ï¼Œä¸”å¤§ç›®æ¨™æˆ–å€å¡Šæ¨™é¡Œæœ‰æ˜ç¢ºåŸºæº–å¹´ï¼Œè«‹**ç¹¼æ‰¿**è©²åŸºæº–å¹´ã€‚
   - **æœ€å¾Œæ‰‹æ®µ**: è‹¥ä»¥ä¸Šçš†ç„¡ï¼Œæ‰è€ƒæ…®ä½¿ç”¨ Progress_History ä¸­æœ€æ—©é‚£å¹´ - 1ã€‚

# Output JSON Schema
è«‹è¼¸å‡ºä¸€å€‹ JSON Listï¼š
[
  {{
    "Report_Year": {current_year},
"Standardized_Focus_Area": "String (e.g., 'Packaging', 'Climate')",
    "Standardized_Metric": "String (e.g., 'Recycled Content')",
    
    // Level 3: é©ç”¨ç¯„ç–‡/æè³ª
    // è¦å‰‡ï¼š
    // 1. å„ªå…ˆå¾è¨»è…³æˆ–æ¨™é¡Œæå–ä¸»è¦ç¯„ç–‡ (å¦‚ "Primary plastic packaging")ã€‚
    // 2. è‹¥ Original_Goal_Text ä¸­æ˜ç¢ºæåŠè¡¡é‡æ–¹å¼ (å¦‚ "absolute tonnage", "per serving")ï¼Œè«‹å‹™å¿…è£œå……åœ¨æ‹¬è™Ÿå…§ã€‚
    // ç¯„ä¾‹è¼¸å…¥: "Reduce absolute tonnage... of primary plastic" -> è¼¸å‡º: "Primary plastic packaging (absolute tonnage)"
    "Scope": "String",
    
    "Original_Goal_Text": "String (ä¿ç•™å ±å‘Šä¸­çš„å®Œæ•´åŸå§‹æè¿°)",
    "Target_Deadline": Number (e.g., 2025, 2030),
    "Target_Value": "String (e.g., '25%', '50%', 'Net Zero')",
    
    // Baseline æå–æ³¨æ„ï¼šè«‹å‹™å¿…æª¢æŸ¥ä¸Šæ¨™(superscript)å°æ‡‰çš„è¨»è…³ï¼Œæˆ–å¤§æ¨™é¡Œçš„åŸºæº–å¹´è¨­å®š
    "Baseline_Year": "String (è‹¥æœ‰æåŠåŸºæº–å¹´å‰‡å¡«å…¥ï¼Œå¦å‰‡ null)",
    "Progress_History": [
       {{ "Year": Number, "Value": "String" }}
    ]
  }}
]

# Begin Extraction
è«‹åˆ†æä»¥ä¸‹å…§å®¹ï¼š
{content}
"""
    return template




----------- .\core\risk.py -----------

import ast
from collections import defaultdict
from datetime import datetime
from typing import Any, Dict, List, Tuple

import pandas as pd

from .cleaning import clean_value, clean_year


def calculate_risk(
    json_data: List[Dict[str, Any]],
) -> Tuple[pd.DataFrame, List[Dict[str, Any]]]:
    """
    æ ¸å¿ƒé¢¨éšªè¨ˆç®—å‡½å¼
    æ”¯æ´: ç·šæ€§é æœŸæ³•ã€è·é›¢ç›®æ¨™æ³•ã€çµ•å°å€¼å‹•æ…‹è½‰ç™¾åˆ†æ¯”

    åƒæ•¸:
        json_data: ç”± LLM è¼¸å‡ºçš„ç›®æ¨™è³‡æ–™åˆ—è¡¨

    å›å‚³:
        (åˆ†æçµæœ DataFrame, è­¦å‘Šåˆ—è¡¨)
    """
    results: List[Dict[str, Any]] = []
    warnings: List[Dict[str, Any]] = []  # è¿½è¹¤éœ€è¦é¡¯ç¤ºè­¦å‘Šçš„è¨˜éŒ„

    # --- é è™•ç†ï¼šç‚ºæ¯ç­†è³‡æ–™åˆ¤æ–·æ˜¯å¦èˆ‡åŒçµ„çš„å‰ä¸€ç­† target ä¸åŒ ---
    change_notes_by_index: Dict[int, str] = {}
    entries: List[Dict[str, Any]] = []
    for idx, it in enumerate(json_data):
        f = it.get("Standardized_Focus_Area", "Unknown")
        m = it.get("Standardized_Metric", "Unknown")
        s = it.get("Scope", "Global")
        norm_s = (
            it.get("Normalize_Scope")
            or it.get("Normalized_Scope")
            or it.get("NormalizedScope")
            or it.get("Standardized_Scope")
            or s
        )
        ry = clean_year(it.get("Report_Year"))
        ty = clean_year(it.get("Target_Deadline"))
        tv = it.get("Target_Value")
        by = clean_year(it.get("Baseline_Year"))
        entries.append(
            {
                "idx": idx,
                "focus": f,
                "metric": m,
                "norm_scope": norm_s,
                "report_year": ry,
                "target_year": ty,
                "target_val": tv,
                "baseline_year": by,
            }
        )

    groups: Dict[Any, List[Dict[str, Any]]] = defaultdict(list)
    for e in entries:
        groups[(e["focus"], e["metric"], e["norm_scope"])].append(e)

    for _, lst in groups.items():
        # å…ˆæŒ‰ Report_Year å‡åºæ’åˆ—ï¼Œç¼ºå¹´è€…æ”¾åˆ°æœ€å¾Œï¼ˆç¶­æŒåŸå§‹é †åºï¼‰
        lst_with_year = [e for e in lst if e["report_year"] is not None]
        lst_no_year = [e for e in lst if e["report_year"] is None]
        lst_sorted = sorted(lst_with_year, key=lambda x: x["report_year"]) + lst_no_year
        for i in range(1, len(lst_sorted)):
            prev = lst_sorted[i - 1]
            cur = lst_sorted[i]
            prev_ty = prev.get("target_year")
            prev_tv = prev.get("target_val")
            cur_ty = cur.get("target_year")
            cur_tv = cur.get("target_val")
            prev_by = prev.get("baseline_year")
            cur_by = cur.get("baseline_year")
            # å…ˆæ¯”è¼ƒ targetï¼ˆdeadline æˆ– valueï¼‰ï¼Œè‹¥ target ç›¸åŒå†æ¯”è¼ƒ baseline year
            if (prev_ty != cur_ty) or (prev_tv != cur_tv):
                change_notes_by_index[
                    cur["idx"]
                ] = f"; ç›®æ¨™å·²è®Šæ›´ (å‰: {prev_ty}å¹´ {prev_tv} -> ç¾: {cur_ty}å¹´ {cur_tv})"
                if prev_by != cur_by:
                    change_notes_by_index[cur["idx"]] += (
                        f"; åŸºæº–å¹´å·²è®Šæ›´ (å‰: {prev_by} -> ç¾: {cur_by})"
                    )
            elif prev_by != cur_by:
                change_notes_by_index[
                    cur["idx"]
                ] = f"åŸºæº–å¹´å·²è®Šæ›´ (å‰: {prev_by} -> ç¾: {cur_by})"

    # --- ä¸»è¦è¨ˆç®—æµç¨‹ ---
    for idx, item in enumerate(json_data):
        try:
            # --- A. åŸºç¤è³‡æ–™è®€å– ---
            focus_area = item.get("Standardized_Focus_Area", "Unknown")
            metric = item.get("Standardized_Metric", "Unknown")
            scope = item.get("Scope", "Global")
            report_year = clean_year(item.get("Report_Year"))

            # è®€å–ç›®æ¨™ (Target)
            target_year = clean_year(item.get("Target_Deadline"))
            target_val_str = item.get("Target_Value")
            # å„ªå…ˆå˜—è©¦å¯èƒ½çš„æ¨™æº–åŒ– scope æ¬„ä½
            norm_scope = (
                item.get("Normalize_Scope")
                or item.get("Normalized_Scope")
                or item.get("NormalizedScope")
                or item.get("Standardized_Scope")
                or scope
            )
            _ = norm_scope  # ä¿ç•™è®Šæ•¸ä»¥åˆ©æ—¥å¾Œæ“´å……

            # å¾é è™•ç†çµæœä¸­å–å¾—è®Šæ›´å‚™è¨»ï¼ˆè‹¥æœ‰ï¼‰
            change_note = change_notes_by_index.get(idx, "")

            # è®€å–åŸºæº–å¹´ (Baseline)
            base_year = clean_year(item.get("Baseline_Year"))

            # ç›®æ¨™é€šå¸¸æ˜¯ç™¾åˆ†æ¯”ï¼Œå¼·åˆ¶è¦–ç‚ºç™¾åˆ†æ¯”è™•ç†
            target_reduction, _, _ = clean_value(target_val_str)

            # å¦‚æœç›®æ¨™æ²’å¯«%ï¼Œä½†æ•¸å€¼æ¯”å¦‚æ˜¯ 20ï¼Œé€šå¸¸æŒ‡ 20% (0.2)
            if target_reduction is not None and target_reduction > 1:
                target_reduction /= 100

            # --- B. è§£æé€²åº¦æ­·å² (Progress History) ---
            history_str = item.get("Progress_History", "[]")
            try:
                if isinstance(history_str, list):
                    history_list = history_str
                else:
                    history_list = ast.literal_eval(history_str)
            except Exception:
                history_list = []

            if not history_list:
                results.append(
                    {
                        "Focus_Area": focus_area,
                        "Metric": metric,
                        "Report_Year": report_year,
                        "Risk_Level": "æ•¸æ“šä¸è¶³",
                        "Analysis_Note": "ç„¡æ­·å²é€²åº¦æ•¸æ“š",
                        "Target": f"{target_year}å¹´ {target_val_str}",
                        "Has_Negative_Warning": False,
                        "Target_Change_Note": change_note,
                    }
                )
                continue

            # æ•´ç†æ­·å²æ•¸æ“š
            history_map: Dict[int, Dict[str, Any]] = {}
            valid_history: List[Dict[str, Any]] = []
            has_negative_warning = False  # è¿½è¹¤æ˜¯å¦æœ‰è² æ•¸è­¦å‘Š

            for h in history_list:
                y = clean_year(h.get("Year"))
                raw_v = h.get("Value")
                v, is_pct, is_negative_fmt = clean_value(raw_v)

                if y is not None and v is not None:
                    record = {
                        "Year": y,
                        "Value": v,
                        "Is_Pct": is_pct,
                        "Raw": raw_v,
                        "Is_Negative_Fmt": is_negative_fmt,
                    }
                    valid_history.append(record)
                    history_map[y] = record
                    # å¦‚æœæœ€æ–°å¹´ä»½æœ‰è² æ•¸æ ¼å¼è­¦å‘Š
                    if is_negative_fmt:
                        has_negative_warning = True

            if not valid_history:
                results.append(
                    {
                        "Focus_Area": focus_area,
                        "Metric": metric,
                        "Report_Year": report_year,
                        "Scope": scope,
                        "Risk_Level": "æ•¸æ“šä¸è¶³",
                        "Note": "ç„¡æ­·å²é€²åº¦æ•¸æ“š",
                        "Target": f"{target_year}å¹´ {target_val_str}",
                        "Current_Status": "N/A",
                        "Has_Negative_Warning": False,
                        "Target_Change_Note": change_note,
                        "Analysis_Note": "",
                    }
                )
                continue

            valid_history.sort(key=lambda x: x["Year"])
            latest_record = valid_history[-1]
            Y_current = latest_record["Year"]

            # å¦‚æœç¼ºå°‘åŸºæº–å¹´ï¼Œä½†æœ‰æ­·å²æ•¸æ“šï¼Œé¡¯ç¤ºè©²å¹´åº¦çš„æ¸›é‡ç‹€æ³
            if base_year is None:
                actual_reduction = latest_record["Value"]
                results.append(
                    {
                        "Focus_Area": focus_area,
                        "Metric": metric,
                        "Report_Year": report_year,
                        "Scope": scope,
                        "Risk_Level": "æ•¸æ“šä¸è¶³",
                        "Target": f"{target_year}å¹´ {target_val_str}",
                        "Current_Status": f"{Y_current}å¹´ (æ¸›é‡ {actual_reduction:.1%})"
                        if actual_reduction is not None
                        else "N/A",
                        "Has_Negative_Warning": False,
                        "Target_Change_Note": change_note,
                        "Analysis_Note": "ç„¡æ³•è¨ˆç®—é¢¨éšªï¼ˆç¼ºå°‘åŸºæº–å¹´ï¼‰",
                    }
                )
                continue

            # --- C. è¨ˆç®—å¯¦éš›æ¸›é‡ (Actual Reduction) ---
            actual_reduction = 0.0
            calc_method = ""

            # åˆ¤æ–·æ˜¯ç”¨ã€Œçµ•å°å€¼ã€ç®—é‚„æ˜¯ç›´æ¥æ‹¿ã€Œç™¾åˆ†æ¯”ã€
            if not latest_record["Is_Pct"]:
                # æƒ…å¢ƒ 1: æ­·å²æ•¸æ“šæ˜¯ã€Œçµ•å°æ•¸å€¼ã€(Absolute Value)
                if base_year in history_map:
                    base_val = history_map[base_year]["Value"]
                    curr_val = latest_record["Value"]

                    if base_val != 0:
                        # å…¬å¼: (åŸºæº– - ç¾åœ¨) / åŸºæº–
                        actual_reduction = (base_val - curr_val) / base_val
                        calc_method = (
                            f"çµ•å°å€¼è¨ˆç®— (åŸºæº–{base_year}: {base_val:,.0f} "
                            f"-> {Y_current}: {curr_val:,.0f})"
                        )
                    else:
                        results.append(
                            {
                                "Focus_Area": focus_area,
                                "Metric": metric,
                                "Report_Year": report_year,
                                "Risk_Level": "æ•¸æ“šéŒ¯èª¤",
                                "Analysis_Note": "åŸºæº–å¹´æ’æ”¾é‡ç‚º 0",
                                "Has_Negative_Warning": False,
                                "Target_Change_Note": change_note,
                            }
                        )
                        continue
                else:
                    results.append(
                        {
                            "Focus_Area": focus_area,
                            "Metric": metric,
                            "Report_Year": report_year,
                            "Risk_Level": "æ•¸æ“šä¸è¶³",
                            "Target": f"{target_year}å¹´ {target_val_str}",
                            "Analysis_Note": (
                                "æ­·å²æ•¸æ“šç‚ºçµ•å°å€¼ï¼Œä½†åœ¨ History ä¸­æ‰¾ä¸åˆ°åŸºæº–å¹´ "
                                f"({base_year}) çš„æ•¸æ“šã€‚"
                            ),
                            "Has_Negative_Warning": False,
                            "Target_Change_Note": change_note,
                        }
                    )
                    continue
            else:
                # æƒ…å¢ƒ 2: æ­·å²æ•¸æ“šæœ¬èº«å°±æ˜¯ã€Œæ¸›é‡ç™¾åˆ†æ¯”ã€
                actual_reduction = latest_record["Value"]
                calc_method = "ç›´æ¥è®€å–ç™¾åˆ†æ¯”"

            # --- D. æ ¸å¿ƒæ¼”ç®—æ³• (Risk Logic) ---
            total_years = target_year - base_year
            elapsed_years = Y_current - base_year

            if total_years <= 0:
                results.append(
                    {
                        "Focus_Area": focus_area,
                        "Metric": metric,
                        "Report_Year": report_year,
                        "Risk_Level": "è¨­å®šéŒ¯èª¤",
                        "Analysis_Note": "ç›®æ¨™å¹´æ—©æ–¼åŸºæº–å¹´",
                        "Has_Negative_Warning": False,
                        "Target_Change_Note": change_note,
                    }
                )
                continue

            elapsed_years = max(0, elapsed_years)

            # æ–¹æ³•ä¸€ï¼šç·šæ€§é æœŸé€²åº¦æ³•
            expected_progress = (elapsed_years / total_years) * target_reduction

            if expected_progress and expected_progress > 0:
                gap = (expected_progress - actual_reduction) / expected_progress
            else:
                gap = 0

            flag1 = gap > 0.1  # è½å¾Œ 10% ä»¥ä¸Š
            flag3 = gap > 1.0  # è½å¾Œ 100% ä»¥ä¸Š

            # æ–¹æ³•äºŒï¼šè·é›¢ç›®æ¨™æ³•
            time_ratio = elapsed_years / total_years if total_years else 0
            target_ratio = (
                actual_reduction / target_reduction if target_reduction and target_reduction > 0 else 0
            )

            flag2 = time_ratio >= 0.5 and target_ratio < 0.5

            # --- E. é¢¨éšªåˆ¤å®š ---
            if (flag1 and flag2) or flag3:
                risk_level = "ğŸ”´ é«˜åº¦é¢¨éšª"
            elif flag1 or flag2:
                risk_level = "ğŸŸ  ä¸­åº¦é¢¨éšª"
            else:
                risk_level = "ğŸŸ¢ ä½é¢¨éšª"

            # --- F. ç”¢ç”Ÿå‚™è¨» ---
            if risk_level.startswith("ğŸŸ¢"):
                note = f"é€²åº¦ç¬¦åˆé æœŸã€‚{calc_method}"
            else:
                note = (
                    f"æ‡‰æ¸› {expected_progress:.1%}, å¯¦æ¸› {actual_reduction:.1%} "
                    f"(Gap: {gap:.1%})ã€‚ {calc_method}"
                )

            result_item = {
                "Focus_Area": focus_area,
                "Report_Year": report_year,
                "Metric": metric,
                "Scope": scope,
                "Target": f"{target_year}å¹´ {target_val_str}",
                "Current_Status": f"{Y_current}å¹´ (æ¸›é‡ {actual_reduction:.1%})",
                "Risk_Level": risk_level,
                "Analysis_Note": note,
                "Has_Negative_Warning": has_negative_warning and actual_reduction < 0,
                "Target_Change_Note": change_note,
            }
            results.append(result_item)

            # å¦‚æœæœ‰è² æ•¸è­¦å‘Šï¼Œæ·»åŠ åˆ°è­¦å‘Šåˆ—è¡¨
            if result_item["Has_Negative_Warning"]:
                warnings.append(
                    {
                        "Focus_Area": focus_area,
                        "Metric": metric,
                        "Year": Y_current,
                        "Status": actual_reduction,
                    }
                )

        except Exception as e:  # noqa: BLE001
            results.append(
                {
                    "Focus_Area": item.get("Standardized_Focus_Area"),
                    "Metric": item.get("Standardized_Metric"),
                    "Report_Year": item.get("Report_Year"),
                    "Risk_Level": "è¨ˆç®—éŒ¯èª¤",
                    "Note": str(e),
                    "Current_Status": "N/A",
                    "Target": "N/A",
                    "Analysis_Note": "N/A",
                    "Scope": "N/A",
                    "Has_Negative_Warning": False,
                }
            )

    return pd.DataFrame(results), warnings




----------- .\memory-bank\architecture.md -----------

### å°ˆæ¡ˆæ¶æ§‹ç¸½è¦½

æœ¬å·¥å…·çš„ç›®æ¨™æ˜¯å°‡ ESG å ±å‘Šå¾ **PDF â†’ Markdown â†’ LLM JSON â†’ é¢¨éšªè¨ˆç®—çµæœ** ä¸²æˆä¸€æ¢ä¹¾æ·¨ã€å¯ç¶­è­·çš„è³‡æ–™æµï¼›
åŒæ™‚ä¹Ÿæä¾›ä¸€æ¢ **PDF â†’ LLM JSON (Vision) â†’ é¢¨éšªè¨ˆç®—çµæœ** çš„ CLI æµç¨‹ï¼Œç”¨ä¾†ç›´æ¥å¾åœ–è¡¨é æ“·å–ç›®æ¨™èˆ‡æ­·å²è¶¨å‹¢æ•¸æ“šã€‚

- **UI Layer (`ui/` + `app.py`)**  
  - è² è²¬æ‰€æœ‰ Streamlit äº’å‹•ã€æª”æ¡ˆä¸Šå‚³ã€æŒ‰éˆ•èˆ‡é ç±¤åˆ‡æ›ã€‚
  - ä¸å¯«å•†æ¥­é‚è¼¯ï¼Œåªå‘¼å« core æ¨¡çµ„ã€‚
- **Core Layer (`core/`)**  
  - æ”¾æ‰€æœ‰èˆ‡ ESG/æ•¸æ“šè™•ç†ç›¸é—œçš„ã€Œç´”é‚è¼¯ã€ï¼šæ¸…æ´—ã€é¢¨éšªè¨ˆç®—ã€Prompt ç”Ÿæˆã€PDF æ··åˆå…§å®¹æŠ½å–èˆ‡ Gemini ä¸²æ¥ã€‚
  - é€™å±¤ç†è«–ä¸Šå¯ä»¥è¢« CLIã€APIã€Batch Job é‡è¤‡ä½¿ç”¨ã€‚

---

### æª”æ¡ˆèˆ‡æ¨¡çµ„è·è²¬

- **`app.py`**  
  - Streamlit çš„å…¥å£æª”ï¼ˆ`streamlit run app.py`ï¼‰ã€‚  
  - å»ºç«‹ 3 å€‹é ç±¤ï¼Œåˆ†åˆ¥å§”æ´¾åˆ° `ui/tab_pdf_to_md.py`ã€`ui/tab_generate_prompt.py`ã€`ui/tab_risk_assessment.py` ä¾†æ¸²æŸ“å…§å®¹ã€‚  
  - ç†æƒ³æƒ…æ³ï¼šé€™è£¡ä¸å†å‡ºç¾ä»»ä½•æ¸…æ´—é‚è¼¯æˆ–è¤‡é›œæ¼”ç®—æ³•ï¼Œåªåšã€Œç‰ˆé¢é…ç½® + å‘¼å«æ¨¡çµ„ã€ã€‚ç›®å‰ä»ä¿ç•™èˆŠç‰ˆå¯¦ä½œï¼Œå¯é€æ­¥æ”¶æ–‚æˆåƒ…å‘¼å« UI æ¨¡çµ„ã€‚

#### Core Layer

- **`core/cleaning.py`**
  - `clean_year(value)`ï¼šå°‡ä»»æ„è¼¸å…¥ï¼ˆå­—ä¸²ã€floatã€Noneï¼‰è½‰ç‚ºå¹´ä»½ `int`ï¼Œå¤±æ•—å› `None`ã€‚
  - `clean_value(value)`ï¼šé€šç”¨æ•¸å€¼ parserã€‚  
    - æ”¯æ´æœƒè¨ˆè² æ•¸ `(5)` â†’ `-5`ã€‚  
    - é¿å…æŠ“åˆ°å–®ä½ä¸­çš„æ•¸å­—ï¼ˆä¾‹å¦‚ `tCO2` çš„ `2`ï¼‰ã€‚  
    - è‡ªå‹•è¾¨è­˜ç™¾åˆ†æ¯”ä¸¦è½‰ç‚º 0â€“1 é–“å°æ•¸ã€‚  
  - `normalize_packaging_scope(scope)`ï¼šå°‡èˆ‡åŒ…è£/å¡‘è† ç›¸é—œçš„æè¿°ï¼ˆvirgin / recycled / reusable...ï¼‰å°æ‡‰åˆ°æ¨™æº–åŒ– Scope tagã€‚
  - æ­¤æ¨¡çµ„ä¸ä¾è³´ Streamlitï¼Œå¯å–®ç¨ç”¨æ–¼ batch è™•ç†æˆ–æ¸¬è©¦ã€‚

- **`core/risk.py`**
  - `calculate_risk(json_data)`ï¼šæ•´å€‹ ESG ç›®æ¨™çš„é¢¨éšªè¨ˆç®—æ ¸å¿ƒã€‚  
  - è¼¸å…¥ï¼šLLM ä¾ç…§ Prompt schema ç”¢å‡ºçš„ JSON listã€‚  
  - ä¸»è¦æ­¥é©Ÿï¼š  
    1. ä¾ `Standardized_Focus_Area + Standardized_Metric + Scope` åˆ†çµ„ï¼ŒåŒçµ„å…§æ¯”å°ç›®æ¨™èˆ‡åŸºæº–å¹´çš„è®Šæ›´ï¼Œç”¢å‡º `Target_Change_Note`ã€‚  
    2. å°æ¯ç­†ç›®æ¨™è§£æ `Progress_History`ï¼Œç”¨ `clean_year`/`clean_value` æ¸…æ´—ã€‚  
    3. å¦‚æœæ­·å²æ˜¯ã€Œçµ•å°å€¼ã€ï¼Œä¾ `baseline` â†’ `current` è¨ˆç®—æ¸›é‡ç™¾åˆ†æ¯”ï¼›è‹¥æœ¬èº«æ˜¯ç™¾åˆ†æ¯”å‰‡ç›´æ¥ä½¿ç”¨ã€‚  
    4. ç”¨ **ç·šæ€§é æœŸé€²åº¦æ³•** + **è·é›¢ç›®æ¨™æ³•** å…©ç¨®æ–¹æ³•ç®—è½å¾Œç¨‹åº¦ï¼Œè¼¸å‡º `Risk_Level`ï¼ˆğŸ”´/ğŸŸ /ğŸŸ¢ï¼‰èˆ‡èªªæ˜æ–‡å­—ã€‚  
    5. è‹¥ç™¼ç¾æœƒè¨ˆè² æ•¸ä¸”å¯¦éš›æ¸›é‡ < 0ï¼Œæ¨™è¨˜ç‚º `Has_Negative_Warning`ï¼Œä¸¦åœ¨ UI å±¤å½ˆå‡ºè­¦å‘Šã€‚  
  - å›å‚³ï¼š`(pandas.DataFrame, warnings_list)`ã€‚DataFrame çµ¦ UI é¡¯ç¤º/ä¸‹è¼‰ï¼Œwarnings çµ¦ UI é¡¯ç¤ºç´…è‰² alertã€‚

- **`core/pdf_extractor.py` + `pp.py`**
  - `pp.extract_content_smart(pdf_path, page_num)`ï¼šå–®é æ™ºæ…§æŠ½å–å™¨ã€‚  
    - å…ˆç”¨ PyMuPDF è®€å–é é¢æ–‡å­—ï¼›è‹¥æ–‡å­—é•·åº¦ `< 1000`ï¼Œè¦–ç‚ºã€Œæƒæ/å¤§åœ–é ã€ï¼Œç›´æ¥æŠŠæ•´é æ¸²æŸ“æˆé«˜è§£æ PNGï¼Œä¸¦åœ¨å›å‚³æ–‡å­—ä¸­æ¨™è¨»ç‚ºæƒææç¤ºã€‚  
    - è‹¥æ–‡å­—è¶³å¤ ï¼Œç•¶ä½œä¸€èˆ¬æ–‡å­—é ï¼šä¿ç•™ `page.get_text()` çš„å…¨æ–‡ï¼ŒåŒæ™‚æŠ“å–é é¢ä¸­æ‰€æœ‰å…§åµŒåœ–ç‰‡ bytesã€‚  
  - `core.pdf_extractor.extract_mixed_content(pdf_path)`ï¼šå¤šé æ··åˆå…§å®¹æå–å™¨ã€‚  
    - é€é å‘¼å« `extract_content_smart`ï¼Œçµ„æˆä¸€å€‹ listï¼Œæ¯é åŒ…å«ï¼š`page_index`, `text`, `images`, `is_scanned`ã€‚  
    - ç‚ºäº†ç¢ºä¿ LLM çœŸçš„ã€Œçœ‹åˆ°ã€æ‰€æœ‰å‘é‡åœ–è¡¨ï¼Œå°æ–¼ **éæƒæé  (`is_scanned=False`)**ï¼Œé¡å¤–å†è£œä¸€å¼µæ•´é  screenshotï¼ˆä¸­ç­‰è§£æåº¦ PNGï¼‰æ”¾é€² `images`ã€‚  
    - æœ€çµ‚ä¿è­‰ï¼šæ¯ä¸€é é€é€² Vision çš„ `images` ä¸­éƒ½è‡³å°‘æœ‰ä¸€å¼µã€Œå®Œæ•´é é¢ã€çš„æˆªåœ–ï¼Œé¿å…åœ–è¡¨è¢« PDF å‘é‡æ ¼å¼åƒæ‰ã€‚

- **`core/gemini_client.py`**
  - `GeminiClient`ï¼šå°è£ Google Gemini 1.5/2.x Flash/Lite æ¨¡å‹ï¼ˆé€é `google-generativeai`ï¼‰ã€‚  
    - åœ¨ `__init__` å…§ä½¿ç”¨ `python-dotenv` è¼‰å…¥ `.env`ï¼Œå¾ `GOOGLE_API_KEY` å–å¾—é‡‘é‘°ä¸¦å‘¼å« `genai.configure()`ã€‚  
    - `_describe_images(images)`ï¼šä½¿ç”¨ Vision æ¨¡å‹å…ˆæŠŠåœ–è¡¨/è¶¨å‹¢åœ–è½‰æˆçµæ§‹åŒ–æè¿°æ–‡å­—ï¼ˆç‰¹åˆ¥å¼·èª¿ã€Œå¹´åº¦ + æ•¸å€¼ã€èˆ‡ baseline/æ¸›é‡ç™¾åˆ†æ¯”ï¼‰ï¼Œè¼¸å‡ºæœƒé•·å¾—åƒï¼š  
      - `2005: 2,567 è¬å™¸`, `2006: 2,710 è¬å™¸`, `æ¯” 2007 å¹´ 3,182 è¬å™¸æ¸›å°‘ 22%` â€¦ ç­‰ã€‚  
    - `extract_goals_from_page(page_text, images, current_year)`ï¼š  
      1. å…ˆå‘¼å« `_describe_images`ï¼Œå–å¾—æ‰€æœ‰åœ–ç‰‡ä¸­çš„ç›®æ¨™å€¼ + æ­·å²è¶¨å‹¢æ–‡å­—ã€‚  
      2. å°‡ã€ŒåŸå§‹é é¢æ–‡å­—ã€èˆ‡ã€ŒVision ç”¢ç”Ÿçš„åœ–è¡¨æè¿°ã€åˆä½µç‚ºä¸€æ®µ `content`ï¼Œä¸Ÿçµ¦ `core.prompt.get_audit_prompt` ç”¢ç”Ÿé•· Promptã€‚  
      3. å‘¼å« Geminiï¼Œè¨­å®š `response_mime_type="application/json"`ï¼Œå¼·åˆ¶ç”¢ç”Ÿç¬¦åˆ Schema çš„ JSON listã€‚  
      4. å°å›å‚³çµæœåš `json.loads` èˆ‡å®¹éŒ¯è™•ç†ï¼ˆå–®ç‰©ä»¶/åŒ…åœ¨ `items`ã€`data` ç­‰ key ä¸‹éƒ½èƒ½è™•ç†ï¼‰ï¼Œæœ€å¾Œå›å‚³ `List[Dict]`ã€‚

- **`core/prompt.py`**
  - `get_audit_prompt(current_year, content)`ï¼šç”¢ç”Ÿçµ¦ LLMï¼ˆChatGPT/Claude/Gemini ç­‰ï¼‰çš„é•· promptã€‚  
  - å…§å®¹åŒ…å«ï¼š  
    - Role & Contextï¼ˆç¨½æ ¸å“¡è§’è‰² + å ±å‘Šå¹´ä»½ï¼‰ã€‚  
    - æ¨™æº–åŒ– ESG å­—å…¸ï¼ˆFocus Area / Metric / Scopeï¼‰ã€‚  
    - è³‡æ–™æ¸…æ´—è¦å‰‡ï¼ˆ**å¼·åˆ¶æŠ½å–æ­·å²è¶¨å‹¢è‡³ `Progress_History`**ï¼Œä»¥åŠ deadline/baseline æ¨è«–é‚è¼¯ï¼‰ï¼š  
      - å°ˆé–€ä¸€æ¢è¦å‰‡æè¿°ï¼šç•¶åœ–è¡¨æˆ–è¡¨æ ¼ä¸­å‡ºç¾ã€Œå¹´åº¦ + æ•¸å€¼ã€çš„è¶¨å‹¢ç·šæˆ–é•·æ¢åœ–ï¼ˆä¾‹å¦‚ 2005~2030 å¹´æ’æ”¾é‡è¶¨å‹¢ï¼‰ï¼ŒLLM å¿…é ˆç›¡å¯èƒ½æŠŠæ‰€æœ‰å¯è¾¨è­˜çš„å¹´åº¦èˆ‡æ•¸å€¼æŠ½å‡ºä¾†ï¼Œå¡«å…¥ `Progress_History`ï¼ˆå³ä½¿å¹´ä»½æ—©æ–¼ç•¶å‰å¹´ä»½ä¹Ÿä¸èƒ½å¿½ç•¥ï¼‰ã€‚  
      - å…¶ä»–è¦å‰‡å‰‡è² è²¬å¾åŸæ–‡åˆ¤æ–· Target å¹´ä»½ã€Baseline å¹´ä»½èˆ‡ Target å€¼ï¼ˆä¾‹å¦‚ã€Œæ¯” 2007 å¹´æ¸›å°‘ 22%ã€ï¼‰ã€‚  
    - æœ€çµ‚è¼¸å‡º JSON Schemaã€‚  
    - ä¸¦æŠŠå¯¦éš›çš„ Markdown å…§å®¹ (`content`) å¤¾åœ¨æœ€å¾Œçµ¦ LLM è§£æã€‚
  - é€™å€‹å‡½å¼æ˜¯ã€Œå–®ç´” string æ¨¡æ¿ã€ï¼Œä¸ä¾è³´ Streamlitã€‚

#### UI Layer

- **`ui/tab_pdf_to_md.py`**
  - Tab1ï¼šã€Œå ±å‘Šè½‰æ› (PDF â†’ Markdown)ã€ã€‚  
  - æµç¨‹ï¼š  
    1. ä½¿ç”¨ `st.file_uploader` ä¸Šå‚³ PDFã€‚  
    2. ç”±æª”åçŒœæ¸¬é è¨­å¹´ä»½ï¼ˆ`20xx` regexï¼‰ã€‚  
    3. ä½¿ç”¨ `MarkItDown` å°‡ PDF è½‰æˆ Markdown æ–‡å­—ã€‚  
    4. çµæœå¯«å…¥ `st.session_state.markdown_content` èˆ‡ `st.session_state.report_year`ã€‚  
    5. æä¾› Markdown é è¦½èˆ‡ `.md` ä¸‹è¼‰ã€‚  
  - æ­¤ tab åƒ…è² è²¬ **I/O èˆ‡ç‹€æ…‹ç´€éŒ„**ï¼Œä¸è™•ç†å¾ŒçºŒ LLM æˆ–é¢¨éšªè¨ˆç®—ã€‚

- **`ui/tab_generate_prompt.py`**
  - Tab2ï¼šã€Œç”¢ç”Ÿç¨½æ ¸ Promptã€ã€‚  
  - å¾ `st.session_state.markdown_content` & `report_year` è®€å–è³‡æ–™ï¼Œå‘¼å« `core.prompt.get_audit_prompt`ã€‚  
  - é¡¯ç¤ºå®Œæ•´ Prompt ä¾›è¤‡è£½ï¼Œä¸¦æä¾› `.txt` ä¸‹è¼‰ã€‚  
  - è‹¥å°šæœªå®Œæˆ Tab1 è½‰æ›ï¼Œæœƒé¡¯ç¤ºæç¤ºè¨Šæ¯ã€‚

- **`ui/tab_risk_assessment.py`**
  - Tab3ï¼šã€Œç¸¾æ•ˆè¿½è¹¤èˆ‡é¢¨éšªè©•ä¼°ã€ã€‚  
  - æµç¨‹ï¼š  
    1. ä¸Šå‚³ LLM è¼¸å‡ºçš„ JSON æª”ï¼ˆæ ¼å¼éœ€ç¬¦åˆ Prompt å®šç¾©ï¼‰ã€‚  
    2. å‘¼å« `core.risk.calculate_risk` æ‹¿åˆ° DataFrame èˆ‡è­¦å‘Šåˆ—è¡¨ã€‚  
    3. é‡å° `Has_Negative_Warning` çš„ç´€éŒ„ä»¥ `st.error` é¡¯ç¤ºç´…è‰²è­¦å‘Šã€‚  
    4. å°‡çµæœä»¥ `st.dataframe` å‘ˆç¾ï¼Œä¸¦è¼¸å‡ºæˆ CSV ä¸‹è¼‰ã€‚  
  - UI éƒ¨åˆ†ä»ç¶­æŒåŸæœ¬çš„æ’åºã€æ¬„ä½éš±è—é‚è¼¯ï¼Œåªæ˜¯å•†æ¥­é‚è¼¯è¢«æ¬åˆ° coreã€‚

---

### è³‡æ–™æµ (Data Flow)

1. **PDF â†’ Markdownï¼ˆäº’å‹•å¼ UI æµç¨‹ï¼‰**
   - ä½¿ç”¨è€…åœ¨ Tab1 ä¸Šå‚³ PDFã€‚  
   - `ui/tab_pdf_to_md.py` å‘¼å« `MarkItDown` æŠ½å–æ–‡å­—ï¼Œå¯«å…¥ `st.session_state.markdown_content`ã€‚  
   - åŒæ™‚è¨˜éŒ„ `report_year`ã€‚

2. **Markdown â†’ LLM Prompt**
   - Tab2 è®€å– `markdown_content` + `report_year`ã€‚  
   - å‘¼å« `core.prompt.get_audit_prompt()` ç”¢å‡ºé•· promptã€‚  
   - ä½¿ç”¨è€…å°‡æ­¤ prompt è²¼çµ¦ LLMï¼Œå–å¾— JSON æª”æ¡ˆã€‚

3. **LLM JSON â†’ é¢¨éšªåˆ†æ**
   - ä½¿ç”¨è€…åœ¨ Tab3 ä¸Šå‚³ JSONã€‚  
   - `ui/tab_risk_assessment.py` å‘¼å« `core.risk.calculate_risk(json_data)`ã€‚  
   - é¢¨éšªçµæœï¼ˆåŒ…å« Gapã€é æœŸé€²åº¦ã€å¯¦éš›é€²åº¦èˆ‡ Target_Change_Noteï¼‰ä»¥è¡¨æ ¼èˆ‡ CSV å½¢å¼å‘ˆç¾ã€‚

4. **PDF â†’ LLM JSONï¼ˆVision + CLI æµç¨‹ï¼‰**
   - ä½¿ç”¨è€…åœ¨å‘½ä»¤åˆ—åŸ·è¡Œï¼š  
     - `python esg_goal_miner.py --pdf <å ±å‘ŠPDF> --year <å ±å‘Šå¹´ä»½> --output <è¼¸å‡ºJSONè·¯å¾‘>`ã€‚  
   - `esg_goal_miner.py`ï¼š  
     1. å‘¼å« `core.pdf_extractor.extract_mixed_content`ï¼Œé€é å–å¾— `{ page_index, text, images, is_scanned }`ã€‚  
     2. å°æ¯ä¸€é å‘¼å« `GeminiClient.extract_goals_from_page`ï¼Œå°‡æ–‡å­— + åœ–ç‰‡ï¼ˆåŒ…å«æ•´é  screenshotï¼‰é€é€² Gemini Vision + Text æ¨¡å‹ã€‚  
     3. æ”¶é›†æ‰€æœ‰é é¢çš„ JSON çµæœï¼Œçµ±ä¸€å¯«æˆä¸€å€‹ `.json` æª”ï¼ˆæ¯ç­†è‡³å°‘åŒ…å« `Report_Year`, `Standardized_Focus_Area`, `Standardized_Metric`, `Scope`, `Original_Goal_Text`, `Target_Deadline`, `Target_Value`, `Baseline_Year`, `Progress_History`ï¼‰ã€‚  
   - å¾ŒçºŒè‹¥è¦åšé¢¨éšªåˆ†æï¼Œå¯ç›´æ¥å°‡æ­¤ JSON æª”ä¸Ÿé€² UI Tab3 æˆ–åœ¨ Notebook ä¸­å‘¼å« `core.risk.calculate_risk`ã€‚

---

### æ•ˆèƒ½èˆ‡ç¶­è­·æ€§å„ªåŒ–èªªæ˜

- å°‡æ¸…æ´—èˆ‡è¨ˆç®—é‚è¼¯é›†ä¸­åˆ° `core/`ï¼Œé¿å…åœ¨ UI ä¸­é‡è¤‡æ’°å¯«é‚è¼¯ï¼Œä¹‹å¾Œè‹¥è¦æ”¹æ¼”ç®—æ³•åªéœ€æ”¹ä¸€è™•ã€‚  
- `calculate_risk` å…ˆåšä¸€æ¬¡åˆ†çµ„èˆ‡ target/baseline è®Šæ›´åµæ¸¬ï¼Œå†é€²å…¥ä¸»è¨ˆç®—è¿´åœˆï¼Œæ¸›å°‘é‡è¤‡é‹ç®—èˆ‡åˆ¤æ–·ã€‚  
- Streamlit å±¤å°ˆæ³¨åœ¨ç‹€æ…‹ï¼ˆ`st.session_state`ï¼‰èˆ‡ I/Oï¼Œæœªä¾†è‹¥è¦æ”¹æˆ API æœå‹™ï¼Œåªéœ€é‡ç”¨ core æ¨¡çµ„å³å¯ã€‚  
- UI æ¨¡çµ„å½¼æ­¤ç¨ç«‹ï¼Œæ–°å¢ç¬¬ 4 å€‹ tabï¼ˆä¾‹å¦‚åœ–è¡¨è¦–è¦ºåŒ–ï¼‰æ™‚ï¼Œåªéœ€è¦æ–°å»ºä¸€å€‹ `ui/tab_xxx.py` ä¸¦åœ¨ `app.py` æ–°å¢ä¸€å€‹ tab å‘¼å«ã€‚




----------- .\memory-bank\progress.md -----------

### æª”æ¡ˆé€²åº¦èˆ‡è¨­è¨ˆç†ç”±ç¸½è¦½

æ­¤æ–‡ä»¶ç´€éŒ„ç›®å‰æ¯å€‹æª”æ¡ˆã€Œåœ¨å¹¹å˜›ã€ä»¥åŠã€Œç‚ºä»€éº¼è¦é€™æ¨£è¨­è¨ˆã€ï¼Œæ–¹ä¾¿æœªä¾†è‡ªå·±æˆ–å…¶ä»–é–‹ç™¼è€…æ¥æ‰‹ã€‚

---

### 1. `app.py`

- **ç›®å‰ç‹€æ…‹**
  - åŸå§‹ç‰ˆæœ¬ï¼šåŒæ™‚åŒ…å«æ‰€æœ‰æ¸…æ´—é‚è¼¯ã€é¢¨éšªè¨ˆç®—ã€Prompt æ¨¡æ¿èˆ‡ä¸‰å€‹ Streamlit åˆ†é  UIã€‚  
  - è¨­è¨ˆç›®æ¨™ï¼šé€æ­¥æ”¶æ–‚ç‚ºã€Œåªåšé ç±¤é…ç½®èˆ‡å‘¼å« UI æ¨¡çµ„ã€çš„è–„å…¥å£ã€‚

- **ä¸‹ä¸€æ­¥å»ºè­°é‡æ§‹**
  - å·²æ–°å¢ `core/` èˆ‡ `ui/` æ¨¡çµ„ï¼Œå»ºè­°ä½ æ¥ä¸‹ä¾†å°‡ï¼š  
    - é¢¨éšªè¨ˆç®—ç›¸é—œçš„ `calculate_risk` å…¨æ•¸ç§»åˆ° `core/risk.py`ï¼ˆç›®å‰å·²å­˜åœ¨å¼·åŒ–ç‰ˆï¼Œå¯æ”¹ç‚ºå¾è©²æ¨¡çµ„åŒ¯å…¥ï¼‰ã€‚  
    - æ•¸å€¼èˆ‡å¹´ä»½æ¸…æ´— `clean_year`ã€`clean_value`ã€`normalize_packaging_scope` æ”¹ç”¨ `core/cleaning.py`ã€‚  
    - Tab1/2/3 çš„ UI å…§å®¹ï¼Œæ”¹ç‚ºå‘¼å« `ui/tab_*.py` ä¸­çš„ `render()` å‡½å¼ï¼Œè®“ `app.py` åªä¿ç•™ï¼š  
      ```python
      tab1, tab2, tab3 = st.tabs([...])
      with tab1: tab_pdf_to_md.render()
      ```

- **è¨­è¨ˆç†ç”±**
  - ä¿ç•™èˆŠå¯¦ä½œå¯ä»¥é™ä½ä¸€æ¬¡æ€§å¤§æ”¹çš„é¢¨éšªï¼Œä½ å¯ä»¥åœ¨æœ¬åœ°é€æ­¥æ¸¬è©¦ã€ç¢ºèªæ–° core / ui æ¨¡çµ„ç©©å®šå¾Œï¼Œå†è®“ `app.py` å®Œå…¨åˆ‡æ›åˆ°æ–°æ¶æ§‹ã€‚

---

### 2. `core/cleaning.py`

- **åœ¨å¹¹å˜›**
  - é›†ä¸­æ‰€æœ‰ã€Œé€šç”¨å‰è™•ç†ã€ï¼š  
    - `clean_year` çµ±ä¸€è™•ç†å„ç¨®å¹´ä»½æ ¼å¼ã€‚  
    - `clean_value` çµ±ä¸€è™•ç†æ•¸å€¼ã€ç™¾åˆ†æ¯”ã€æœƒè¨ˆè² æ•¸ã€é¿å…å–®ä½é»æ•¸å­—ã€‚  
    - `normalize_packaging_scope` æ•´ç†åŒ…è£ç›¸é—œ scope tagã€‚

- **ç‚ºä»€éº¼é€™æ¨£è¨­è¨ˆ**
  - é€™äº›å‡½å¼èˆ‡ UI å®Œå…¨ç„¡é—œï¼Œåªè·Ÿè³‡æ–™æœ‰é—œï¼Œé©åˆç¨ç«‹åœ¨ core å±¤ï¼Œæ–¹ä¾¿ï¼š  
    - å–®å…ƒæ¸¬è©¦ã€‚  
    - å…¶ä»–æœå‹™ï¼ˆå¦‚æ‰¹æ¬¡è…³æœ¬ã€APIï¼‰é‡ç”¨ã€‚

---

### 3. `core/risk.py`

- **åœ¨å¹¹å˜›**
  - `calculate_risk(json_data)`ï¼šçµ±ä¸€ç®¡ç†æ‰€æœ‰ ESG ç›®æ¨™é¢¨éšªè¨ˆç®—é‚è¼¯ã€‚  
  - å°‡èˆŠç‰ˆ `app.py` ä¸­çš„æ ¸å¿ƒæ¼”ç®—æ³•æŠ½å‡ºä¸¦åŠ ä¸Šå‹åˆ¥è¨»è¨˜èˆ‡éŒ¯èª¤è™•ç†ï¼Œä½¿å…¶æ›´ç©©å®šã€å¯é‡è¤‡ä½¿ç”¨ã€‚

- **ç‚ºä»€éº¼é€™æ¨£è¨­è¨ˆ**
  - é¢¨éšªæ¼”ç®—æ³•æ˜¯æ¥­å‹™æ ¸å¿ƒï¼Œä¸æ‡‰è©²æ•£è½åœ¨ UI ç¨‹å¼ç¢¼è£¡ã€‚  
  - åˆ†é›¢å¾Œï¼š  
    - å¯ä»¥å¾ˆå®¹æ˜“åœ¨ Jupyter Notebookã€CLI æˆ–å¾Œç«¯æœå‹™ä¸­é‡ç”¨åŒä¸€å€‹ `calculate_risk`ã€‚  
    - æœªä¾†è‹¥è¦æ”¹æ¼”ç®—æ³•ï¼ˆä¾‹å¦‚åŠ ä¸Šæ–°çš„ flag æˆ– scoringï¼‰ï¼Œåªéœ€æ”¹é€™ä¸€æ”¯ã€‚

---

### 4. `core/prompt.py`

- **åœ¨å¹¹å˜›**
  - `get_audit_prompt(current_year, content)`ï¼šå°è£ LLM Prompt æ¨¡æ¿çš„ç”¢ç”Ÿé‚è¼¯ã€‚
  - æŠŠèˆŠ `app.py` è£¡é•·é•·çš„ f-string æŠ½å‡ºä¾†ï¼Œè®Šæˆä¹¾æ·¨çš„ä¸€å€‹å‡½å¼ã€‚

- **ç‚ºä»€éº¼é€™æ¨£è¨­è¨ˆ**
  - Prompt æœ¬è³ªä¸Šæ˜¯ã€Œæ–‡å­—æ¨¡æ¿ã€ï¼Œå’Œ UI æˆ–è³‡æ–™è¨ˆç®—é‚è¼¯ä¸åŒã€‚  
  - æŠ½åˆ° core ä¹‹å¾Œï¼š  
    - å¯ä»¥åœ¨ CLI æˆ–å…¶ä»–ä»‹é¢ç›´æ¥å‘¼å« `get_audit_prompt`ï¼Œä¸ç”¨ä¾è³´ Streamlitã€‚  
    - æœªä¾†è‹¥è¦é‡å°ä¸åŒå®¢æˆ¶ï¼æ¨™æº–ï¼ˆå¦‚ SBTi / TCFDï¼‰èª¿æ•´ Promptï¼Œå¯ä»¥å¢åŠ å¤šå€‹å‡½å¼æˆ–åƒæ•¸ï¼Œè€Œä¸æ±¡æŸ“ UI ç¨‹å¼ç¢¼ã€‚

---

### 5. `ui/tab_pdf_to_md.py`

- **åœ¨å¹¹å˜›**
  - è² è²¬ Tab1ï¼šã€Œå ±å‘Šè½‰æ› (PDF â†’ Markdown)ã€çš„æ‰€æœ‰ç•«é¢èˆ‡æµç¨‹ã€‚  
  - åªè™•ç†ï¼šæª”æ¡ˆä¸Šå‚³ã€å¹´ä»½è¼¸å…¥ã€å‘¼å« `MarkItDown` è½‰æª”ã€å¯«å…¥ `st.session_state`ã€é¡¯ç¤ºé è¦½èˆ‡æä¾›ä¸‹è¼‰ã€‚

- **ç‚ºä»€éº¼é€™æ¨£è¨­è¨ˆ**
  - è®“ã€Œä¸€å€‹æª”æ¡ˆå°æ‡‰ä¸€å€‹é ç±¤ã€ï¼Œé™ä½è€¦åˆã€‚  
  - è‹¥æœªä¾†è¦å¢åŠ æ›´å¤šè½‰æª”æ–¹å¼ï¼ˆä¾‹å¦‚æ”¯æ´ DOCXã€HTMLï¼‰ï¼Œåªæ”¹é€™ä¸€æ”¯ï¼Œä¸å½±éŸ¿é¢¨éšªè¨ˆç®—èˆ‡ Promptã€‚

---

### 6. `ui/tab_generate_prompt.py`

- **åœ¨å¹¹å˜›**
  - è² è²¬ Tab2ï¼šã€Œç”¢ç”Ÿç¨½æ ¸ Promptã€çš„ç•«é¢ã€‚  
  - å¾ `session_state` å– Markdown + å¹´ä»½ï¼Œå‘¼å« `core.prompt.get_audit_prompt`ï¼Œé¡¯ç¤ºèˆ‡ä¸‹è¼‰ã€‚

- **ç‚ºä»€éº¼é€™æ¨£è¨­è¨ˆ**
  - UI å±¤åªå°ˆæ³¨åœ¨å±•ç¤ºèˆ‡ä¸‹è¼‰ï¼ŒçœŸæ­£çš„ Prompt çµæ§‹ç•™åœ¨ coreã€‚  
  - è‹¥æœªä¾†è¦å¤šä¸€ç¨® Promptï¼ˆä¾‹å¦‚ã€Œå›æº¯æ­·å²é©—è­‰ç”¨ã€ï¼‰ï¼Œå¯ä»¥åœ¨é€™è£¡å¤šä¸€å€‹é¸é … + èª¿ç”¨ä¸åŒçš„ core å‡½å¼ã€‚

---

### 7. `ui/tab_risk_assessment.py`

- **åœ¨å¹¹å˜›**
  - è² è²¬ Tab3ï¼šã€Œç¸¾æ•ˆè¿½è¹¤èˆ‡é¢¨éšªè©•ä¼°ã€ã€‚  
  - ä¸Šå‚³ JSON â†’ å‘¼å« `core.risk.calculate_risk` â†’ é¡¯ç¤º DataFrame + ä¸‹è¼‰ CSV â†’ å°è² å‘é€²åº¦é¡¯ç¤ºç´…è‰²è­¦ç¤ºã€‚

- **ç‚ºä»€éº¼é€™æ¨£è¨­è¨ˆ**
  - æ¼”ç®—æ³•èˆ‡ UI æ˜ç¢ºåˆ†é›¢ï¼š  
    - ç•¶ä½ æƒ³æ”¹ UIï¼ˆä¾‹å¦‚åŠ ä¸Šç¯©é¸ã€å¤šèªè¨€åˆ‡æ›ï¼‰æ™‚ï¼Œä¸æœƒèª¤å‹•åˆ°é¢¨éšªæ¼”ç®—æ³•ã€‚  
    - ç•¶ä½ æƒ³æ”¹æ¼”ç®—æ³•ï¼Œä¹Ÿä¸ç”¨å‹•åˆ°ä»»ä½• Streamlit å…ƒä»¶ã€‚

---

### å¾ŒçºŒå»ºè­°ï¼ˆRoadmapï¼‰

- **çŸ­æœŸ**
  - å°‡ `app.py` é€æ­¥æ”¹å¯«ç‚ºåªåŒ¯å…¥ `ui/` æ¨¡çµ„ä¸¦å‘¼å« `render()`ã€‚  
  - åˆªé™¤ `app.py` è£¡å·²è¢«æŠ½å‡ºåˆ° `core/` çš„å‡½å¼ï¼Œé¿å…é‡è¤‡é‚è¼¯ã€‚  

- **ä¸­æœŸ**
  - ç‚º `core/cleaning.py` èˆ‡ `core/risk.py` å¢åŠ ç°¡å–®çš„å–®å…ƒæ¸¬è©¦ï¼Œç¢ºä¿æœªä¾†æ”¹å‹•ä¸æœƒç ´å£æ¼”ç®—æ³•ã€‚  
  - æŠŠå…±ç”¨å¸¸æ•¸ï¼ˆä¾‹å¦‚ emojiã€æ–‡å­—æ¨™ç±¤ï¼‰é›†ä¸­åˆ°ä¸€å€‹ `constants.py`ã€‚  

- **é•·æœŸ**
  - å°‡ core å°è£æˆå¯è¢«å…¶ä»–å°ˆæ¡ˆ import çš„å°å‹ packageï¼ŒStreamlit åƒ…æ˜¯å…¶ä¸­ä¸€å€‹å‰ç«¯ä»‹é¢ã€‚  
  - è‹¥æœ‰éœ€è¦ï¼Œå¯ä»¥æ–°å¢ä¸€å€‹ `api/` å±¤ï¼ˆFastAPI / Flaskï¼‰è®“å¤–éƒ¨ç³»çµ±èƒ½å‘¼å«åŒä¸€å¥—é¢¨éšªè¨ˆç®—é‚è¼¯ã€‚




----------- .\ui\__init__.py -----------

"""
Streamlit UI åˆ†é æ¨¡çµ„åŒ…ï¼š
- tab_pdf_to_md: PDF -> Markdown è½‰æ›æµç¨‹
- tab_generate_prompt: Prompt ç”¢ç”Ÿæµç¨‹
- tab_risk_assessment: JSON -> é¢¨éšªåˆ†ææµç¨‹
"""




----------- .\ui\tab_generate_prompt.py -----------

import streamlit as st

from core.prompt import get_audit_prompt


def render() -> None:
    """Tab 2: ç”¢ç”Ÿç¨½æ ¸ Prompt"""
    st.header("æ­¥é©ŸäºŒï¼šç”Ÿæˆ AI ç¨½æ ¸ Prompt")
    st.markdown("å°‡è½‰æ›å¾Œçš„å…§å®¹çµåˆæ¨™æº–åŒ–æŒ‡ä»¤ï¼Œç”¢ç”Ÿå¯ä¾› ChatGPT/Claude/Gemini ä½¿ç”¨çš„ Promptã€‚")

    if st.session_state.get("markdown_content"):
        final_prompt = get_audit_prompt(
            st.session_state.get("report_year", 2024),
            st.session_state["markdown_content"],
        )

        st.info(
            "ğŸ’¡ è«‹è¤‡è£½ä¸‹æ–¹å…§å®¹ï¼Œè²¼çµ¦ LLM æ¨¡å‹ï¼Œä¸¦å°‡å…¶å›å‚³çš„ JSON å­˜æª”ä¾›æ­¥é©Ÿä¸‰ä½¿ç”¨ã€‚"
        )
        st.text_area("Prompt é è¦½", final_prompt, height=400, key="prompt_preview")

        st.download_button(
            label="ä¸‹è¼‰å®Œæ•´ Prompt (.txt)",
            data=final_prompt,
            file_name=f"Audit_Prompt_{st.session_state.get('report_year', 2024)}.txt",
            mime="text/plain",
        )
    else:
        st.warning("è«‹å…ˆåœ¨æ­¥é©Ÿä¸€ä¸Šå‚³ä¸¦è½‰æ› PDF å ±å‘Šã€‚")




----------- .\ui\tab_pdf_to_json.py -----------

import json
import re
import tempfile
from pathlib import Path
from typing import Any, Dict, List, Optional, Set

import streamlit as st

from core.gemini_client import GeminiClient
from core.pdf_extractor import extract_mixed_content


def _infer_year_from_name(name: str, default: int = 2024) -> int:
    match = re.search(r"20\d{2}", name)
    if match:
        try:
            return int(match.group(0))
        except ValueError:
            return default
    return default


def _run_extraction(
    pdf_path: Path, report_year: int, pages_filter: Optional[Set[int]] = None
) -> List[Dict[str, Any]]:
    """ç›´æ¥åœ¨è¨˜æ†¶é«”ä¸­åŸ·è¡Œ PDF â†’ JSON ç›®æ¨™æ“·å–ï¼Œä¸å¯«å…¥å¯¦é«” JSON æª”ã€‚

    pages_filter:
        è‹¥æä¾›ï¼Œåƒ…å°æŒ‡å®šé ç¢¼å‘¼å« Geminiï¼ˆ0-based page indexï¼‰ã€‚
        ä¾‹å¦‚ {0, 4, 5} ä»£è¡¨ç¬¬ 1, 5, 6 é ã€‚
    """
    pages = extract_mixed_content(str(pdf_path))

    if pages_filter:
        pages = [p for p in pages if int(p.get("page_index", -1)) in pages_filter]
    client = GeminiClient()

    all_items: List[Dict[str, Any]] = []
    for page in pages:
        text: str = page["text"]
        images: List[bytes] = page["images"]

        page_items = client.extract_goals_from_page(
            page_text=text,
            images=images,
            current_year=report_year,
        )

        for item in page_items:
            if isinstance(item, dict):
                item.setdefault("Report_Year", report_year)
                all_items.append(item)

    return all_items


def render() -> None:
    """å–®ä¸€é ç±¤ï¼šä¸Šå‚³ PDF â†’ ç›´æ¥ç”¢å‡ºç›®æ¨™ JSONã€‚"""
    st.header("æ­¥é©Ÿä¸€ï¼šä¸Šå‚³ ESG å ±å‘Šä¸¦è‡ªå‹•æ“·å–ç›®æ¨™ (PDF â†’ JSON)")
    st.markdown(
        """
        ä¸Šå‚³ ESG å ±å‘Š PDF å¾Œï¼Œç³»çµ±å°‡ï¼š
        1. é€é è®€å–æ–‡å­—èˆ‡åœ–è¡¨ï¼ˆå«æ•´é æˆªåœ–çµ¦ Vision æ¨¡å‹ï¼‰
        2. ä¾ç…§æ¨™æº–åŒ–å­—å…¸èˆ‡ Schemaï¼Œè‡ªå‹•æ“·å–ã€Œæ‰¿è«¾ç›®æ¨™ã€ä¸¦è¼¸å‡º JSON
        """
    )

    uploaded_pdf = st.file_uploader(
        "ä¸Šå‚³ ESG å ±å‘Šæ›¸ (PDF)", type=["pdf"], key="pdf_uploader_v2"
    )

    # æ¨æ¸¬é è¨­å¹´ä»½
    default_year = 2024
    if uploaded_pdf:
        default_year = _infer_year_from_name(uploaded_pdf.name, default_year)

    report_year = st.number_input(
        "è¨­å®šå ±å‘Šå¹´ä»½",
        min_value=2000,
        max_value=2050,
        value=default_year,
        key="report_year_input_v2",
    )

    if "goal_json" not in st.session_state:
        st.session_state.goal_json = None

    # å¯é¸ï¼šé™åˆ¶è¦è§£æçš„é ç¢¼ï¼Œé™ä½ API æˆæœ¬
    pages_raw = st.text_input(
        "ï¼ˆé¸å¡«ï¼‰åªè§£æç‰¹å®šé ç¢¼ä»¥ç¯€çœ API æˆæœ¬ï¼Œä¾‹å¦‚ï¼š5 æˆ– 3-7,10ï¼ˆä»¥ 1 ç‚ºèµ·å§‹é ï¼‰",
        value="",
        key="pages_filter_v2",
    )

    pages_filter: Optional[Set[int]] = None
    if pages_raw.strip():
        pages_filter = set()
        for part in pages_raw.split(","):
            part = part.strip()
            if not part:
                continue
            if "-" in part:
                try:
                    start_s, end_s = part.split("-", 1)
                    start = int(start_s)
                    end = int(end_s)
                    for p in range(start, end + 1):
                        # è½‰ç‚º 0-based index
                        pages_filter.add(p - 1)
                except ValueError:
                    continue
            else:
                try:
                    p = int(part)
                    pages_filter.add(p - 1)
                except ValueError:
                    continue

    if uploaded_pdf is not None:
        if st.button("é–‹å§‹è§£æç›®æ¨™ (PDF â†’ JSON)"):
            st.info(f"æ­£åœ¨è™•ç†æª”æ¡ˆ: {uploaded_pdf.name} ... é€™å¯èƒ½éœ€è¦æ•¸åç§’ã€‚")

            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_pdf:
                tmp_pdf.write(uploaded_pdf.read())
                tmp_path = Path(tmp_pdf.name)

            try:
                with st.spinner("Gemini æ­£åœ¨è§£æåœ–è¡¨èˆ‡æ–‡å­—..."):
                    data = _run_extraction(tmp_path, int(report_year), pages_filter)
                st.session_state.goal_json = data
                st.success(f"è§£æå®Œæˆï¼å…±æ“·å–åˆ° {len(data)} ç­†ç›®æ¨™ç´€éŒ„ã€‚")
            except Exception as e:  # noqa: BLE001
                st.session_state.goal_json = None
                st.error(f"è§£æéç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            finally:
                try:
                    tmp_path.unlink(missing_ok=True)
                except Exception:
                    pass

    if st.session_state.goal_json:
        st.subheader("ğŸ“„ æŠ½å–å‡ºçš„ç›®æ¨™ JSON")
        pretty = json.dumps(st.session_state.goal_json, ensure_ascii=False, indent=2)
        st.code(pretty, language="json")

        file_name = f"ESG_Targets_{int(report_year)}.json"
        st.download_button(
            label="ğŸ“¥ ä¸‹è¼‰ç›®æ¨™ JSON æª”",
            data=pretty,
            file_name=file_name,
            mime="application/json",
        )




----------- .\ui\tab_pdf_to_md.py -----------

import os
import re
import tempfile

import streamlit as st
from markitdown import MarkItDown


def render() -> None:
    """Tab 1: å ±å‘Šè½‰æ› (PDF -> Markdown)"""
    st.header("æ­¥é©Ÿä¸€ï¼šä¸Šå‚³ä¸¦è½‰æ›å ±å‘Šæ›¸")
    st.markdown("å°‡ PDF æ ¼å¼çš„ ESG å ±å‘Šæ›¸è½‰æ›ç‚º AI å¯è®€çš„ Markdown æ ¼å¼ã€‚")

    uploaded_pdf = st.file_uploader(
        "ä¸Šå‚³ ESG å ±å‘Šæ›¸ (PDF)", type=["pdf"], key="pdf_uploader"
    )

    # ç‹€æ…‹ä¿å­˜ï¼šMarkdown å…§å®¹
    if "markdown_content" not in st.session_state:
        st.session_state.markdown_content = ""

    # å˜—è©¦å¾æª”åè‡ªå‹•æå–å¹´ä»½
    default_year = 2024
    if uploaded_pdf:
        match = re.search(r"20\d{2}", uploaded_pdf.name)
        if match:
            default_year = int(match.group(0))

    report_year = st.number_input(
        "è¨­å®šå ±å‘Šå¹´ä»½",
        min_value=2000,
        max_value=2030,
        value=default_year,
        key="report_year_input",
    )
    st.session_state.report_year = report_year

    if uploaded_pdf is not None:
        if st.button("é–‹å§‹è½‰æ›"):
            st.info(f"æ­£åœ¨è™•ç†æª”æ¡ˆ: {uploaded_pdf.name} ...")

            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_pdf:
                tmp_pdf.write(uploaded_pdf.read())
                tmp_pdf_path = tmp_pdf.name

            try:
                md = MarkItDown()
                result = md.convert(tmp_pdf_path)
                st.session_state.markdown_content = result.text_content
                os.remove(tmp_pdf_path)
                st.success("è½‰æ›æˆåŠŸï¼è«‹è‡³ã€Œç”¢ç”Ÿç¨½æ ¸ Promptã€åˆ†é æŸ¥çœ‹ã€‚")

            except Exception as e:  # noqa: BLE001
                st.error(f"è½‰æ›éŒ¯èª¤: {e}")
                if os.path.exists(tmp_pdf_path):
                    os.remove(tmp_pdf_path)

    if st.session_state.markdown_content:
        with st.expander("æŸ¥çœ‹è½‰æ›å¾Œçš„ Markdown å…§å®¹"):
            st.text_area(
                "å…§å®¹é è¦½", st.session_state.markdown_content, height=300, key="md_preview"
            )
            st.download_button(
                label="ä¸‹è¼‰ Markdown (.md)",
                data=st.session_state.markdown_content,
                file_name=f"report_{report_year}.md",
                mime="text/markdown",
            )




----------- .\ui\tab_risk_assessment.py -----------

import json
from datetime import datetime

import streamlit as st

from core.risk import calculate_risk


def render() -> None:
    """Tab 3: ç¸¾æ•ˆè¿½è¹¤èˆ‡é¢¨éšªè©•ä¼°"""
    st.header("æ­¥é©Ÿä¸‰ï¼šç¸¾æ•ˆè¿½è¹¤èˆ‡é¢¨éšªè©•ä¼°")
    st.markdown(
        """
    è«‹ä¸Šå‚³ç”± LLM ç”¢å‡ºçš„ **çµæ§‹åŒ– JSON æª”æ¡ˆ**ã€‚
    ç³»çµ±å°‡è‡ªå‹•åŸ·è¡Œï¼š
    1. **è§£ææ­·å¹´æ•¸æ“š** (æ”¯æ´çµ•å°å€¼è½‰ç™¾åˆ†æ¯”ï¼Œè‡ªå‹•å°‡ `(5)%` è½‰ç‚º `-5%`)ã€‚
    2. **å°æ‡‰åŸºæº–å¹´** (Baseline Mapping)ã€‚
    3. **è¨ˆç®—é¢¨éšªç­‰ç´š** (ç·šæ€§é æœŸæ³• + è·é›¢ç›®æ¨™æ³•)ã€‚
    """
    )

    uploaded_json = st.file_uploader(
        "ä¸Šå‚³ LLM ç”¢å‡ºçš„ JSON æª”æ¡ˆ", type=["json"], key="json_uploader"
    )

    if uploaded_json is not None:
        try:
            # è®€å– JSON
            json_data = json.load(uploaded_json)
            st.success(f"æˆåŠŸè®€å–æª”æ¡ˆï¼å…± {len(json_data)} ç­†ç›®æ¨™è³‡æ–™ã€‚")

            # åŸ·è¡Œè¨ˆç®—
            with st.spinner("æ­£åœ¨é€²è¡Œé¢¨éšªè©•ä¼°æ¼”ç®—æ³•..."):
                df_result, warnings_list = calculate_risk(json_data)

            # é¡¯ç¤ºè­¦å‘Šå½ˆçª— (Current_Status èƒŒé“è€Œé¦³)
            if warnings_list:
                for warn in warnings_list:
                    st.error(
                        f"âŒå¹´åº¦: {warn['Year']} - {warn['Focus_Area']} - {warn['Metric']}\n\n"
                        f"è©²å¹´åº¦çš„æ¸›é‡ç‹€æ³ç‚ºè² æ•¸ ({warn['Status']:.1%})ï¼Œ"
                        f"èˆ‡ç›®æ¨™èƒŒé“è€Œé¦³ï¼"
                    )

            # é¡¯ç¤ºçµæœ
            st.subheader("ğŸ“Š ç¨½æ ¸åˆ†æçµæœ")

            # ç›´æ¥é¡¯ç¤ºè¡¨æ ¼ï¼ˆéš±è—å…§éƒ¨æ¬„ä½ Has_Negative_Warningï¼‰
            df_display = df_result.drop(columns=["Has_Negative_Warning"], errors="ignore")
            # ä¾ Report_Year é è¨­å‡åºæ’åºï¼ˆè‹¥æœ‰æ­¤æ¬„ä½ï¼‰
            if "Report_Year" in df_display.columns:
                try:
                    df_display = df_display.sort_values(by="Report_Year", ascending=True)
                except Exception:  # noqa: BLE001
                    pass
            st.dataframe(df_display, use_container_width=True)

            # ä¸‹è¼‰ CSV
            df_export = df_result.drop(columns=["Has_Negative_Warning"], errors="ignore")
            csv = df_export.to_csv(index=False, encoding="utf-8-sig")
            base_name = uploaded_json.name.replace(".json", "")
            file_name = (
                f"Audit_Result_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{base_name}.csv"
            )

            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰å®Œæ•´åˆ†æå ±è¡¨ (CSV)",
                data=csv,
                file_name=file_name,
                mime="text/csv",
            )

        except Exception as e:  # noqa: BLE001
            st.error(f"åˆ†æéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            st.info("è«‹ç¢ºèªä¸Šå‚³çš„ JSON æ ¼å¼æ˜¯å¦ç¬¦åˆ Prompt å®šç¾©çš„ Schemaã€‚")
    else:
        st.info("ğŸ‘‹ ç­‰å¾…ä¸Šå‚³ JSON æª”æ¡ˆä¸­...")




